<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>C&plus;&plus; Concurrency</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        <style>
#button {
    display: inline-block;
    background-color: #FF9800;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    /*visibility: hidden;*/
    z-index: 1000;
}
#button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
}
#button:hover {
    cursor: pointer;
    background-color: #333;
}
#button:active {
    background-color: #555;
}
#button.show {
    opacity: 1;
    visibility: visible;
}

#btn-back-to-top {
    position: fixed;
    bottom: 20px;
    right: 20px;
    display: none;
  }

  .to-top {
    background: white;
    position: fixed;
    bottom: 16px;
    right:32px;
    width:50px;
    height:50px;
    border-radius: 50%;
    border-color: white;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size:32px;
    color:#1f1f1f;
    text-decoration: none;
    opacity: 0.5;
    pointer-events: auto;
    transition: all .4s;
    transform: rotate(270deg);
  }
  
</style>
    </head>
    <body class="vscode-body vscode-light">
        <!-- title: C++ Concurrency -->
<h1>C++ Concurrency</h1>
<p><a href="#" class="to-top">➤</i></a></p>
<ul>
<li><a href="#c-concurrency-support">C++ Concurrency Support</a>
<ul>
<li><a href="#c1114">C++11/14</a></li>
<li><a href="#c17">C++17</a></li>
<li><a href="#c20">C++20</a></li>
</ul>
</li>
<li><a href="#reordering">Reordering</a>
<ul>
<li><a href="#x86-has-a-strong-memory-model">x86 has a strong memory model</a></li>
<li><a href="#compiler-barrier">Compiler barrier</a></li>
<li><a href="#memory-barrier">Memory barrier</a></li>
<li><a href="#storeload">#StoreLoad</a></li>
<li><a href="#store-buffer">Store Buffer</a></li>
<li><a href="#x86-implementation">x86 implementation</a>
<ul>
<li><a href="#mfence-httpswwwfelixcloutiercomx86mfence"><code>mfence</code> (https://www.felixcloutier.com/x86/mfence)</a></li>
<li><a href="#sfence-httpswwwfelixcloutiercomx86sfence"><code>sfence</code> (https://www.felixcloutier.com/x86/sfence)</a></li>
<li><a href="#lfence-httpswwwfelixcloutiercomx86lfence"><code>lfence</code> (https://www.felixcloutier.com/x86/lfence)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#memory-order">Memory order</a>
<ul>
<li><a href="#acquire-release-memory-order">acquire-release memory order</a></li>
</ul>
</li>
<li><a href="#measure-threading-cost">Measure Threading Cost</a>
<ul>
<li><a href="#threaded-sequential-access-write-result---each-thread-with-independent-data-set">Threaded sequential access (write) result - each thread with independent data set</a></li>
<li><a href="#concurrent-access-to-shared-data-is-expensive">Concurrent access to shared data is expensive</a></li>
<li><a href="#concurrent-access-to-non-shared-data">Concurrent access to non-shared data</a></li>
</ul>
</li>
</ul>
<h1 id="c-concurrency-support">C++ Concurrency Support</h1>
<h2 id="c1114">C++11/14</h2>
<ul>
<li>It introduced memory model</li>
<li>It introduced the notion of threads
<ul>
<li>Most implementations use system threads to support C++ threads.</li>
</ul>
</li>
<li>It introduced several synchronization primitives
<ul>
<li><code>std::mutex</code>, on POSIX system, it is typically POSIX mutex.</li>
<li>Timed and recursive mutex (following POSIX)</li>
<li><code>std::lock_guard</code>, RAII template for locking/unlocking mutex</li>
<li><code>std::lock()</code> for locking multiple mutexes safely</li>
<li><code>std::condition_variable</code> (following POSIX)</li>
</ul>
</li>
<li>It supports low-level atomic operation: <code>std::atomic</code>
<ul>
<li>CAS</li>
<li>memory order specifiers.</li>
</ul>
</li>
<li>It added support for asynchronous execution - <code>std::async</code> - where a function can be invoked asynchronously (possibly on another thread).
<ul>
<li>It provides limited practical use of parallelism or just execute each asynchronous function in its own thread which is high overhead</li>
</ul>
</li>
</ul>
<h2 id="c17">C++17</h2>
<ul>
<li><a href="c17.html#parallel-stl-alogrithm">Parallel algorithms</a>: The major new feature in C++17</li>
<li><a href="c17.html#stdscoped_lock"><code>std::scoped_lock</code></a>: RAII wrapper for locking multiple mutexes safely without deadlock.</li>
<li>std::shared_mutex
<ul>
<li>a read-writer mutex (following POSIX read-write mutex),</li>
<li>most implementation offer not good performance</li>
</ul>
</li>
<li><a href="c17.html#cache-line-sizes">cache line sizes</a>.</li>
</ul>
<h2 id="c20">C++20</h2>
<ul>
<li><a href="c20.html#corouties">Corouties</a></li>
</ul>
<br>
<br>
<h1 id="reordering">Reordering</h1>
<p>Three levels of reordering</p>
<ul>
<li>compiler;</li>
<li>processor;</li>
<li>memory/cache (such as store buffer)</li>
<li>to programmer, we dont care which level, as long as we guarantee the sequence consistent</li>
</ul>
<p>This document summarises some known mappings of C/C++11 atomic operations to x86: <a href="http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html">http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html</a></p>
<table>
<thead>
<tr>
<th>C/C++11</th>
<th>Operation	x86 implementation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Load Relaxed:</td>
<td>MOV (from memory)</td>
</tr>
<tr>
<td>Load Consume:</td>
<td>MOV (from memory)</td>
</tr>
<tr>
<td>Load Acquire: <br> example: <br> <code>std::atomic&lt;int&gt; x(0);</code> <br> <code>int y = x.load(std::memory_order_acquire);</code></td>
<td>MOV (from memory)</td>
</tr>
<tr>
<td>Load Seq_Cst: <br> example: <br> <code>std::atomic&lt;int&gt; x(0);</code> <br> <code>int y = x.load()</code></td>
<td>MOV (from memory)</td>
</tr>
<tr>
<td>Store Relaxed</td>
<td>MOV (into memory)</td>
</tr>
<tr>
<td>Store Release <br> example: <br> <code>std::atomic&lt;int&gt; x(0);</code> <br> <code>x.store(2, std::memory_order_release);</code></td>
<td>MOV (into memory)</td>
</tr>
<tr>
<td>Store Seq Cst <br> example: <br> <code>std::atomic&lt;int&gt; x(0);</code> <br> <code>x.store(2); or just x = 2;</code></td>
<td>(LOCK) XCHG // alternative: MOV (into memory),MFENCE  <br> The parenthesised (LOCK) reflects the fact that the XCHG instruction on x86 has an <strong>implicit</strong> LOCK prefix. <br>If a compiler emits code using non-temporal stores, it must also emit sufficient fencing to make the usage of non-temporal stores unobservable to callers and callees.</td>
</tr>
<tr>
<td>Consume Fence:</td>
<td>&lt;ignore&gt;</td>
</tr>
<tr>
<td>Acquire Fence:</td>
<td>&lt;ignore&gt;</td>
</tr>
<tr>
<td>Release Fence:</td>
<td>&lt;ignore&gt;</td>
</tr>
<tr>
<td>Acq_Rel Fence:</td>
<td>&lt;ignore&gt;</td>
</tr>
<tr>
<td>Seq_Cst Fence:</td>
<td>MFENCE</td>
</tr>
</tbody>
</table>
<p><strong>Summary:</strong> Code Gen on X86:</p>
<table>
<thead>
<tr>
<th>operation</th>
<th>ordinary</th>
<th>atomic</th>
</tr>
</thead>
<tbody>
<tr>
<td>load:</td>
<td>mov</td>
<td>mov</td>
</tr>
<tr>
<td>store:</td>
<td>mov</td>
<td>xchg</td>
</tr>
<tr>
<td>CaS:</td>
<td>cmpxchg</td>
<td>cmpxchg</td>
</tr>
</tbody>
</table>
<p>From Intel Manual  (Intel 64 and IA-32 Architectures Software Developer’s Manual, Volume 3A):</p>
<ul>
<li>Reads are not reordered with any reads</li>
<li>Writes are not reordered with any writes (some exceptions)</li>
<li>writes are not reordered with older reads</li>
<li>Reads maybe reordered with older writes (different locations)   #storeLoad barrier needed
<ul>
<li>eg, below can be reordered:  <br>
Reads X<br>
writres Y<br>
<img src="images/2023-07-01-12-29-19.png" width="200"></li>
</ul>
</li>
<li>Reads &amp; writes not reordered with locked instruction (such as xchg)
<ul>
<li>means every lock (such as atomic store operation) is a full fence
<img src="images/2023-07-01-20-57-07.png" width="200"></li>
</ul>
</li>
</ul>
<p>POWER (IBM) and ARM v7 architecture have very expensive atomic load operation</p>
<ul>
<li>that's the major reason we have relaxed atomics in c++11 standard</li>
</ul>
<h2 id="x86-has-a-strong-memory-model">x86 has a strong memory model</h2>
<p>A strong hardware memory model is one in which every machine instruction comes implicitly with acquire and release semantics.</p>
<ul>
<li>all reads have acquire semantic</li>
<li>all writes have release semantic
<ul>
<li>As a result, when one CPU core performs a sequence of writes, every other CPU core sees those values change in the same order that they were written.</li>
<li>non-temporal instructions have its own semantics.</li>
</ul>
</li>
<li>This means you get <code>lfence</code> and <code>sfence</code> for free. But <code>lfense</code> + <code>sfence</code> != <code>mfence</code> or full memory barrier. Specifically, for #StoreLoad, you can reorder an acquire-load ahead of a release-store. In fact, it does happen on x86.</li>
<li>all read-modify-write operations have acquire-release semantic.</li>
<li><code>std::memory_order_acq_rel</code> and <code>std::memory_order_seq_cst</code> are the same thing on x86</li>
</ul>
<h2 id="compiler-barrier">Compiler barrier</h2>
<p><a href="https://preshing.com/20120625/memory-ordering-at-compile-time/">https://preshing.com/20120625/memory-ordering-at-compile-time/</a></p>
<p><strong>Direct way:</strong>
The minimalist approach to preventing compiler reordering is by using a special directive known as a compiler barrier</p>
<pre><code class="language-C++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">foo</span><span class="hljs-params">()</span>
</span>{
    A = B + <span class="hljs-number">1</span>;
    <span class="hljs-function"><span class="hljs-keyword">asm</span> <span class="hljs-title">volatile</span><span class="hljs-params">(<span class="hljs-string">&quot;&quot;</span> ::: <span class="hljs-string">&quot;memory&quot;</span>)</span></span>;
    B = <span class="hljs-number">0</span>;
}
</code></pre>
<p><strong>Indirect way:</strong></p>
<ul>
<li>every non-relaxed atomic operation acts as a compiler barrier as well.</li>
<li>every function containing a compiler barrier must act as a compiler barrier itself, even when the function is inlined.</li>
<li>The majority of function calls act as compiler barriers, whether they contain their own compiler barrier or not.
<ul>
<li>This excludes inline functions, and cases where link-time code generation is used</li>
<li>Other than those cases, a call to an external function is even stronger than a compiler barrier, since the compiler has no idea what the function’s side effects will be. Eg., it needs to reload the variable from the memory even the variable is used before the call</li>
</ul>
</li>
</ul>
<p><strong>Out-of-thin-air store:</strong></p>
<p>In particular, compilers were free to introduce stores to shared memory in cases where there previously was none.</p>
<pre><code class="language-C++"><span class="hljs-type">int</span> A, B;
<span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">foo</span><span class="hljs-params">()</span>
</span>{
    <span class="hljs-keyword">if</span> (A)
        B++;
}
<span class="hljs-comment">// Can be optimized to </span>
<span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">foo</span><span class="hljs-params">()</span>
</span>{
    <span class="hljs-keyword">register</span> <span class="hljs-type">int</span> r = B;    <span class="hljs-comment">// Promote B to a register before checking A.</span>
    <span class="hljs-keyword">if</span> (A)
        r++;
    B = r;          <span class="hljs-comment">// Surprise! A new memory store where there previously was none.</span>
}
</code></pre>
<p>In any case, the new C++11 standard explicitly prohibits such behavior from the compiler in cases where it would introduce a data race. The wording can be found in and around §1.10.22 of the <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2012/n3337.pdf">most recent C++11 working draft</a>:</p>
<ul>
<li>Compiler transformations that introduce assignments to a potentially shared memory location that would not be modified by the abstract machine are generally precluded by this standard.</li>
</ul>
<h2 id="memory-barrier">Memory barrier</h2>
<p>Memory barriers control how changes to memory made by one CPU become visible to other CPUs. This is a global control across all CPUs. In low level, memory barriers are implemented by the hardware, it can be invoked by processor-specific instructions. C++11 provides standard memory barriers via <code>std::memory_order_*</code>, which are the modifiers of atomic operations.</p>
<p>You can enforce correct memory ordering on the processor by issuing any instruction which acts as a memory barrier. In some ways, this is the only technique you need to know, because when you use such instructions, compiler ordering is taken care of automatically. Examples of instructions which act as memory barriers include (but are not limited to) the following:</p>
<ul>
<li>Certain inline assembly directives in GCC, such as the PowerPC-specific <code>asm volatile(&quot;lwsync&quot; ::: &quot;memory&quot;)</code></li>
<li>Any Win32 Interlocked operation, except on Xbox 360</li>
<li>Many operations on C++11 atomic types, such as <code>load(std::memory_order_acquire)</code></li>
<li>Operations on POSIX mutexes, such as <code>pthread_mutex_lock</code></li>
</ul>
<p>Each type of memory barrier is named after the type of memory reordering it’s designed to prevent: for example, #StoreLoad is designed to prevent the reordering of a store followed by a load.</p>
<p><img src="images/2022-04-26-14-48-13.png" alt=""></p>
<p><strong>#LoadLoad</strong> is useful when you want to prevent reordering between reads. This is especially useful when a thread is consuming some data depending on a flag e.g. <code>isPublished</code> written by another thread, so reading the data always happens after checking the flag.</p>
<p>Similarly <strong>#StoreStore</strong> fence is used to prevent reordering writes (<code>sfense</code> on x86). This is useful when a thread is publishing some data using a flag, so writing the data always happens before setting the flag.</p>
<p><strong>#LoadStore</strong> fence disallows reordering a subsequent store ahead of the load instruction, which typically happens on CPU when the later store is a cache hit and the earlier load is a cache miss.</p>
<p><strong>#StoreLoad</strong> <strong>is unique</strong> and it acts as a full sync (full memory barrier, <code>mfence</code> on x86) – all writes before the fence will be made visible to all other caches and all reads after the fence will get values at-or-later than the execution time of the fence. If all operations have #StoreLoad fences, it would be sequential consistent.</p>
<h2 id="storeload">#StoreLoad</h2>
<p>A StoreLoad barrier ensures that all stores performed before the barrier are visible to other processors, and that all loads performed after the barrier receive the latest value that is visible at the time of the barrier. In other words, it effectively prevents reordering of all stores before the barrier against all loads after the barrier, respecting the way a sequentially consistent multiprocessor would perform those operations.</p>
<p>#StoreLoad is unique. On most processors, instructions that act as a #StoreLoad barrier tend to be more expensive than instructions acting as the other barrier types.</p>
<p>On x86/64 processors, there is no specific instruction which acts only as a StoreLoad barrier, but there are several instructions which do that and more. The <code>mfence</code> instruction is a full memory barrier, which prevents memory reordering of any kind. In GCC, it can be implemented as follows:</p>
<pre><code class="language-C++">   <span class="hljs-keyword">for</span> (;;)                                  <span class="hljs-comment">// Loop indefinitely</span>
    {
        <span class="hljs-built_in">sem_wait</span>(&amp;beginSema1);                <span class="hljs-comment">// Wait for signal from main thread</span>
        <span class="hljs-keyword">while</span> (random.<span class="hljs-built_in">integer</span>() % <span class="hljs-number">8</span> != <span class="hljs-number">0</span>) {}  <span class="hljs-comment">// Add a short, random delay</span>

        <span class="hljs-comment">// ----- THE TRANSACTION! -----</span>
        X = <span class="hljs-number">1</span>;
        <span class="hljs-function"><span class="hljs-keyword">asm</span> <span class="hljs-title">volatile</span><span class="hljs-params">(<span class="hljs-string">&quot;mfence&quot;</span> ::: <span class="hljs-string">&quot;memory&quot;</span>)</span></span>;  <span class="hljs-comment">// Prevent memory reordering</span>
        r1 = Y;

        <span class="hljs-built_in">sem_post</span>(&amp;endSema);                   <span class="hljs-comment">// Notify transaction complete</span>
    }
</code></pre>
<p>Again, you can verify its presence by looking at the assembly code listing.</p>
<pre><code class="language-asm">   ...
    mov    DWORD PTR _X, 1
    mfence
    mov    eax, DWORD PTR _Y
    mov    DWORD PTR _r1, eax
    ...
</code></pre>
<p>Good article: <a href="https://blog.the-pans.com/std-atomic-from-bottom-up/">https://blog.the-pans.com/std-atomic-from-bottom-up/</a></p>
<h2 id="store-buffer">Store Buffer</h2>
<ul>
<li>The store buffer is a FIFO, and buffers stores until it must flush the stores</li>
<li>size of store buffer is limited, normally it is 32.</li>
<li>the store buffer is serializing if it is full, preventing further store instructions from entering the pipeline</li>
</ul>
<p>When P is trying to write to its cache and its cache state is &quot;invalid&quot;, it needs to first fetch the cacheline from main memory (or transfer from other caches) before it can set exclusive ownership of the cacheline and set its state to &quot;modified&quot;. This can be very slow, hence the introduction of Store Buffer.</p>
<p>In this case, writes get buffered in the cache-local store buffer after issuing read-invalidates to all caches. The cacheline will be eventually updated with the written value after it's fetched from main memory. During this window, read from the cache will have to search for its store buffer for buffered writes. It works for the local cache but not others.</p>
<img src="images/2023-07-01-21-13-31.png" width="600">
<h2 id="x86-implementation">x86 implementation</h2>
<h3 id="mfence-httpswwwfelixcloutiercomx86mfence"><code>mfence</code> (<a href="https://www.felixcloutier.com/x86/mfence">https://www.felixcloutier.com/x86/mfence</a>)</h3>
<p>Performs a serializing operation on all load-from-memory and store-to-memory instructions that were issued prior the MFENCE instruction. This serializing operation guarantees that every load and store instruction that precedes the MFENCE instruction in program order becomes globally visible before any load or store instruction that follows the MFENCE instruction. The MFENCE instruction is ordered with respect to all load and store instructions, other MFENCE instructions, any LFENCE and SFENCE instructions...</p>
<p>In practice, it drains the store buffer before any loads (globally) can proceed.</p>
<h3 id="sfence-httpswwwfelixcloutiercomx86sfence"><code>sfence</code> (<a href="https://www.felixcloutier.com/x86/sfence">https://www.felixcloutier.com/x86/sfence</a>)</h3>
<p>Orders processor execution relative to all memory stores prior to the SFENCE instruction. The processor ensures that every store prior to SFENCE is globally visible before any store after SFENCE becomes globally visible. The SFENCE instruction is ordered with respect to memory stores, other SFENCE instructions, MFENCE instructions, and any serializing instructions (such as the CPUID instruction). It is not ordered with respect to memory loads or the LFENCE instruction.</p>
<p>It doesn't have to drain the store buffer. It's often used for Non-temporal instructions, when by default, operations bypass cache and &quot;breaks&quot; cache coherence explicitly. Inserting sfence after an NT store, makes sure the data becomes visible to other cores.</p>
<h3 id="lfence-httpswwwfelixcloutiercomx86lfence"><code>lfence</code> (<a href="https://www.felixcloutier.com/x86/lfence">https://www.felixcloutier.com/x86/lfence</a>)</h3>
<p>Performs a serializing operation on all load-from-memory instructions that were issued prior the LFENCE instruction. Specifically, LFENCE does not execute until all prior instructions have completed locally, and no later instruction begins execution until LFENCE completes. In particular, an instruction that loads from memory and that precedes an LFENCE receives data from memory prior to completion of the LFENCE.</p>
<ul>
<li>Usage: lfence is mainly used in scenarios where precise load ordering is required. It prevents load operations from being reordered across the fence, but it doesn't affect store operations.</li>
<li>In practice, it drains the local invalidation queue and re-order buffer (load queue). I am not aware of when this is useful on x86.</li>
</ul>
<br>
<br>
<h1 id="memory-order">Memory order</h1>
<p>Curate list ofThreads and memory model for C++
<a href="https://hboehm.info/c++mm/">https://hboehm.info/c++mm/</a></p>
<h2 id="acquire-release-memory-order">acquire-release memory order</h2>
<p>load acquire &lt;-&gt; store release</p>
<p>Acquire-read disallows reordering reads or writes ahead of it, <em>in program order</em>. Release-write disallows reordering reads or writes after it, <em>in program order</em>. It says nothing about fences, which is just implementation detail. In practice it looks like (<a href="https://preshing.com/20120913/acquire-and-release-semantics/">https://preshing.com/20120913/acquire-and-release-semantics/</a>):</p>
<p><img src="images/2022-04-26-15-00-47.png" alt=""></p>
<p>On x86 this is the default behavior without extra memory barriers needed. With weak memory model, reorder can happen.</p>
<p>When an atomic operation is executed with this order, we have a guarantee that</p>
<ul>
<li>any operation that accesses the memory and was executed before the atomic operation becomes visible to another thread before that thread executes an atomic operation on the same atomic variable.</li>
<li>all operations that are executed after the atomic operation become visible only after an atomic operation on the same variable.</li>
</ul>
<p>Caveats:</p>
<ul>
<li>the order is defined relative to the operations both threads execute on the same atomic variable.</li>
<li>both threads must use the acquire-release memory order</li>
<li>all order guarantees are given in terms of before and after the operation on the atomic variable. There are no guarantees on the order of events before the operations, and no guarantees on the order of events after the operations.</li>
</ul>
<p>On the producer side, we need one-half of the guarantee given by the acquire-release memory barrier: release memory order:</p>
<ul>
<li>all operations executed before the atomic operation with the barrier must become visible to other threads before they execute the corresponding atomic operation.</li>
<li>operations executed by this CPU after the atomic operation may become visible in any order, ie., may happen before the other CPU execute the atomic operation.</li>
</ul>
<p>On the consumer side: the acquire memory order:</p>
<ul>
<li>all operations executed after the barrier become visible to other threads after the barrier.</li>
<li>The operations executed by this CPU before the atomic operation may become visible in other CPUs after the atomic operation.</li>
</ul>
<br>
<br>
<h1 id="measure-threading-cost">Measure Threading Cost</h1>
<h2 id="threaded-sequential-access-write-result---each-thread-with-independent-data-set">Threaded sequential access (write) result - each thread with independent data set</h2>
<p><img src="images/2022-01-24-18-11-12.png" alt=""></p>
<p>The left side of the plot. Here, the speed is limited by the L1 cache (up to 32 KB) then by the L2 cache (256 KB). This processor has separate L1 and L2 caches for each core, so, as long as the data fits into the L2 cache, there should not be any interaction between the threads</p>
<p>The right side of the plot - the L3 cache is shared between all the CPU cores. The main memory is shared too.</p>
<ul>
<li>For 1, 2, and even 4 threads, the throughput continues to scale with the number of threads: the main memory appears to have enough bandwidth for up to 4 processors writing into it at full speed.</li>
<li>the throughput almost doesn't increase when we go from 6 to 16 threads. We have saturated the memory bus: it can't write the data any faster.</li>
</ul>
<h2 id="concurrent-access-to-shared-data-is-expensive">Concurrent access to shared data is expensive</h2>
<p><img src="images/2022-01-24-18-12-12.png" alt=""></p>
<p>Even on one thread</p>
<ul>
<li>23 nanoseconds for a mutex-guarded increment.</li>
<li>7 nanoseconds for the atomic increment.
The performance degrades much faster as the number of threads increases.</li>
</ul>
<h2 id="concurrent-access-to-non-shared-data">Concurrent access to non-shared data</h2>
<p><img src="images/2022-01-24-18-13-06.png" alt=""></p>
<p>False-share benchmark</p>
<ul>
<li>On all x86 CPUs, one cache line is 64 bytes. When a CPU needs to lock a memory location for an atomic transaction,it has to lock the entire cache line.</li>
<li>the false sharing appears equivalent to the true data sharing for up to 8 threads but becomes faster for more threads: we are writing 8-byte words, so 8 of them fit into the same cache line. So, if we have, say, 16 threads at any time, there are two threads that can move forward, one for each half of the array.</li>
</ul>
<p>no-sharing benchmark allocates the atomic variables on the stack of each thread. these threads run completely independently of each other.</p>
<p>This analysis shows that the real reason for the high cost of accessing the shared data is the work that must be done to maintain exclusive access to a cache line and to make sure all CPUs have consistent data in their caches.</p>
<ul>
<li>after one CPU has obtained exclusive access and updated even one bit in the cache line, the copy of that line in all other CPUs’ caches is out of date. Before these other CPUs can access any data in the same cache line, they must fetch the updated content from the main memory.</li>
</ul>
<br>
<br>
        
        
    </body>
    </html>