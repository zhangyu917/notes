<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Cache overview</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        <style>
#button {
    display: inline-block;
    background-color: #FF9800;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    /*visibility: hidden;*/
    z-index: 1000;
}
#button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
}
#button:hover {
    cursor: pointer;
    background-color: #333;
}
#button:active {
    background-color: #555;
}
#button.show {
    opacity: 1;
    visibility: visible;
}

#btn-back-to-top {
    position: fixed;
    bottom: 20px;
    right: 20px;
    display: none;
  }

  .to-top {
    background: white;
    position: fixed;
    bottom: 16px;
    right:32px;
    width:50px;
    height:50px;
    border-radius: 50%;
    border-color: white;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size:32px;
    color:#1f1f1f;
    text-decoration: none;
    opacity: 0.5;
    pointer-events: auto;
    transition: all .4s;
    transform: rotate(270deg);
  }
  
</style>
        </head>
        <body class="vscode-body vscode-light">
            <ul>
<li><a href="#cache-overview">Cache overview</a>
<ul>
<li><a href="#write-combine-and-wcb">Write-Combine and WCB</a></li>
<li><a href="#cache-coherence">Cache Coherence</a></li>
</ul>
</li>
<li><a href="#tlb">TLB</a>
<ul>
<li><a href="#tlb-flush">TLB Flush</a></li>
<li><a href="#tlb-shootdowns">TLB shootdowns</a></li>
<li><a href="#hugepage-tlb">Hugepage TLB</a></li>
</ul>
</li>
<li><a href="#page-cache">Page cache</a>
<ul>
<li><a href="#buffer-cache">Buffer Cache</a></li>
<li><a href="#dirty-page-cache-write">Dirty Page Cache Write</a>
<ul>
<li><a href="#pdflush-kernel-thread">pdflush kernel thread</a></li>
<li><a href="#write-back-writable-mmapped-file">write-back writable mmapped file</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#cache-warm">Cache Warm</a>
<ul>
<li><a href="#hardware-prefetch">hardware prefetch</a></li>
</ul>
</li>
<li><a href="#cat---cache-allocation-techonology">CAT - Cache allocation techonology</a>
<ul>
<li><a href="#inclusive-l3">Inclusive L3</a>
<ul>
<li><a href="#skylake-change">Skylake change</a></li>
</ul>
</li>
<li><a href="#introduction-to-cache-allocation-technology-cat">Introduction to Cache Allocation Technology (CAT)</a></li>
</ul>
</li>
<li><a href="#ddio">DDIO</a></li>
<li><a href="#kernel-internals">Kernel Internals</a>
<ul>
<li><a href="#address_space-object">address_space object.</a>
<ul>
<li><a href="#radix-tree-pages">Radix tree pages</a></li>
<li><a href="#find-ptes-using-priority-search-tree-mmap">Find PTEs using priority search tree (mmap)</a></li>
<li><a href="#address_space-operations">address_space operations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a href="#" class="to-top">➤</i></a></p>
<h1 id="cache-overview">Cache overview</h1>
<p>When a process issues the <code>read()</code> system call, it first checks if the data is in the page cache.</p>
<ul>
<li>If it is, the kernel can read the data directly out of RAM. This is called a <em>cache hit</em>.</li>
<li>If the data is not in the cache, called a <em>cache miss</em>, the kernel must schedule block I/O operations to read the data off the disk.</li>
</ul>
<p>In <code>write()</code> system call, caches can implement three strategies</p>
<ul>
<li>no-write cache. data stored in the cache would be written directly to disk, invalidating the cached data and requiring it to be read from disk again on any subsequent read.</li>
<li>write-through cache. A write operation would automatically update both the in-memory cache and the on-disk file.
<ul>
<li>Cache misses on write operations: no-write-allocate policy, the cache miss transaction is sent directly to the lower levels of the
hierarchy, and the block is not loaded into the cache.</li>
</ul>
</li>
<li><strong>write-back cache</strong>. This is what Linux employs. Processes perform write operations directly into the page cache.The backing store (main memory) is not immediately updated. Instead, the written-to pages in the page cache are marked as dirty and are added to a dirty list. Periodically, pages in the dirty list are written back to disk.
<ul>
<li>Recent Intel processors like Core, Xeon, etc. implement multi-level caches that are generally write-back at all levels (L1, L2, L3/LLC).</li>
<li>Write-back is handled by dedicated cache controllers/agents like the CHA (Caching Home Agent) in recent server processors that manage writing back dirty lines to memory.</li>
<li>The <code>WBINVD</code> instruction can be used to write back all modified lines from all levels of the CPU cache hierarchy (including write-back buffers) to main memory and invalidate (flushes) the entire cache hierarchy. <code>WBINVD</code> operates across all cores/processors in the system. <code>WBINVD</code> is a serializing instruction that stalls the CPU pipeline until the write-back and invalidation operations complete. It can be an expensive operation.</li>
<li>Cache misses on write operations: write-allocate policy: the data for the missed location is loaded into the cache from
the lower level of the hierarchy, and the write operation is subsequently handled like a write hit.</li>
</ul>
</li>
</ul>
<p>Some caching techniques:
<a href="https://stackoverflow.com/questions/49092541/which-cache-mapping-technique-is-used-in-intel-core-i7-processor">https://stackoverflow.com/questions/49092541/which-cache-mapping-technique-is-used-in-intel-core-i7-processor</a></p>
<h2 id="write-combine-and-wcb">Write-Combine and WCB</h2>
<ul>
<li>Write-combining is a way to combine and temporarily buffer multiple write operations to memory <strong>within the same cache line</strong> before writing them out to the system bus in a single burst transaction. This avoids having to perform slower partial writes to memory for every individual store operation, improving bandwidth utilization.</li>
<li>It is a weakly-ordered memory type that does not enforce coherency with the CPU caches. Writes can be delayed and combined in the write-combining buffer (WCB).</li>
<li>Intel processors have dedicated write-combining buffers, typically 4-10 cache-line sized (64-byte) buffers per core. On Ice Lake and later processors, the number of WCBs has increased from 10 to 12 per core.</li>
<li>The WCBs are used for writes to memory regions marked as Write-Combining (WC), Uncached Speculative Write Combining (USWC), and regular Writeback (WB) memory types when using non-temporal store instructions.
<ul>
<li>On recent Intel processors, the WCB is also used for writes to regular Writeback (WB) memory when using non-temporal store instructions like <code>MOVNTDQ</code>/<code>MOVNTPS</code>.</li>
</ul>
</li>
</ul>
<h2 id="cache-coherence">Cache Coherence</h2>
<p>Cachce coherence is handled by the hardware (processor). Processor also provides some instructions to operate on cache. In x86, there are below instructions to invalidate/flush the cache.</p>
<ul>
<li>full invaliate/flush
<ul>
<li><code>WBINVD</code>, short for &quot;write back invalidate&quot;. It first writes back all cache contents that are marked as &quot;modified&quot; to memory, then clear the whole cache.</li>
<li><code>INVD</code>, unlike <code>WBINVD</code>, it doesn't do write back, so it doesn't guarantee the data consistency. It is used when there is no need to keep consistency bewteen cache and memory, such as some testing environment.</li>
</ul>
</li>
<li>partial invalidate/flush
<ul>
<li><code>CLFLUSH</code>/<code>CLFLUSHOPT</code>, short for &quot;cache line flush&quot;. It works on the granularity of cache lines, normally used when this cache line doesn't need to be accessed anymore.</li>
</ul>
</li>
</ul>
<br>
<h1 id="tlb">TLB</h1>
<p>The performance of the virtual-memory-to-physical-address lookup is very critical, To facilitate this, most processors implement a <em>translation lookaside buffer</em>, or simply <strong>TLB</strong>, which acts as a <strong>hardware cache</strong> of virtual-to-physical mappings, these mappings are maintained for pages typically of size 4 KiB, 2/4 MiB or 1 GiB.</p>
<ul>
<li>When accessing a virtual address, the processor first checks whether the mapping is cached in the TLB. If there is a hit, the physical address is immediately returned. Otherwise, there is a TLB miss, the page tables are consulted for the corresponding physical address, this is called page walk.
<ul>
<li>To speed up handling of TLB misses, CPUs have a mechanism called <strong>HW page walker</strong>. It performs a page walk directly in HW without interrupting the kernel. High-end processors have several HW page walkers that can handle multiple TLB misses simultaneously.</li>
</ul>
</li>
</ul>
<p><img src="images/2022-02-27-13-08-33.png" alt="">
<img src="images/2022-02-27-13-08-25.png" alt=""></p>
<p>Usually we have L1 ITLB, L1 DTLB, and shared L2 TLB. TLB has fixed entries of PTE, usually it is small. So a TLB miss is “kinda” common. But because each page size is 4K, so even we only have 64 entries in TLB, it can accommodate 4K * 64 data if we have enough locality in the program.</p>
<p>Reference: number of TLB entries</p>
<p>Sandy Bridge:</p>
<table>
<thead>
<tr>
<th>Level</th>
<th>Page Size</th>
<th>Entries</th>
<th>Associativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1 DTLB</td>
<td>4KB</td>
<td>64</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>2MB,4MB</td>
<td>32</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>1GB</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>L2 TLB</td>
<td>4KB</td>
<td>512</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>1GB</td>
<td>NONE</td>
<td></td>
</tr>
</tbody>
</table>
<p>Haswell:</p>
<table>
<thead>
<tr>
<th>Level</th>
<th>Page Size</th>
<th>Entries</th>
<th>Associativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1 DTLB</td>
<td>4KB</td>
<td>64</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>2MB,4MB</td>
<td>32</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>1GB</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>L2 TLB</td>
<td>4KB</td>
<td>1024</td>
<td>8</td>
</tr>
<tr>
<td></td>
<td>1GB</td>
<td>NONE</td>
<td></td>
</tr>
</tbody>
</table>
<p>Skylake:</p>
<table>
<thead>
<tr>
<th>Level</th>
<th>Page Size</th>
<th>Entries</th>
<th>Associativity</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1 DTLB</td>
<td>4KB</td>
<td>64</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>2MB,4MB</td>
<td>32</td>
<td>4</td>
</tr>
<tr>
<td></td>
<td>1GB</td>
<td>4</td>
<td>4</td>
</tr>
<tr>
<td>L2 TLB</td>
<td>4KB</td>
<td>1536</td>
<td>12</td>
</tr>
<tr>
<td></td>
<td>1GB</td>
<td>16</td>
<td>4</td>
</tr>
</tbody>
</table>
<p><strong>TLB miss</strong></p>
<p>Remediation</p>
<ul>
<li>You can minimize TLB misses by reducing your working set size, making sure to pack your data into as few pages as possible.</li>
<li>Additionally, you can utilize larger page sizes than the default 4 KiB. These larger pages are called huge pages and allows you to reference more data using fewer pages.</li>
</ul>
<p><strong>View TLB miss</strong></p>
<pre><code class="language-bash"><span class="hljs-comment"># perf stat -e dTLB-loads,dTLB-load-misses,iTLB-loads,iTLB-load-misses -p $PID</span>
 Performance counter stats <span class="hljs-keyword">for</span> process <span class="hljs-built_in">id</span> <span class="hljs-string">&#x27;21047&#x27;</span>:

           627,809 dTLB-loads
             8,566 dTLB-load-misses          <span class="hljs-comment">#    1.36% of all dTLB cache hits</span>
         2,001,294 iTLB-loads
             3,826 iTLB-load-misses          <span class="hljs-comment">#    0.19% of all iTLB cache hits</span>
</code></pre>
<p><strong>TLB working with Cache</strong></p>
<p>A good video about how TLB and L1 cache work together: <a href="https://www.youtube.com/watch?v=3sX5obQCHNA&amp;ab_channel=DavidBlack-Schaffer">https://www.youtube.com/watch?v=3sX5obQCHNA&amp;ab_channel=DavidBlack-Schaffer</a></p>
<p>L1 Cache is ususally virtually indexed, Physically tagged cache - VIPT</p>
<ul>
<li>L1D caches in most designs take their index bits from below the page offset, and thus are also VIPT allowing TLB lookup to happen in parallel with tag fetch, but without any aliasing problems.</li>
</ul>
<h2 id="tlb-flush">TLB Flush</h2>
<p>Kernel needs to maintain the coherence between TLB and page table (because page table is a structure maintained in the kernel, the mapping between TLB and page table has to be mainteind by kernel, not hardware).</p>
<ul>
<li>When a PTE changes, eg, it is being paged out or swapped out or <code>munmap()</code>'ed, kernel needs to invalidate the TLB entry, this is called <strong>TLB flush</strong>.</li>
<li>As a general rule, any process switch implies changing the set of active page tables. Local TLB entries relative to the old page tables must be flushed.
<ul>
<li>Process switch is indicated by CR3 register change, as CR3 points to process's page table address.</li>
</ul>
</li>
<li>context switch case 1 - system call: process A system call -&gt; kernel mode -&gt; back to process A user mode
<ul>
<li>G (global) bit in TLB entry for entries that don't often change including kernel TLB entries. When CR4 register is set with PGE（page global enable, the TLB entries with G bit set is not flushed.</li>
</ul>
</li>
<li>context switch case 2 - process switch: process A -&gt; kernel mode -&gt; process B
<ul>
<li>In order to avoid frequent tlb flush due to process switch, TLB entry has a TAG called PCID （Process Context Identifiers）. It has 12 bit (read from CR3 PCID), so it can differ 4096 processes at most.</li>
</ul>
</li>
</ul>
<p>Kernel offers several TLB flush methods. Intel microprocessors supports below TLB-invalidating
techniques:</p>
<table>
<thead>
<tr>
<th>Macro name</th>
<th>Description</th>
<th>Used by</th>
</tr>
</thead>
<tbody>
<tr>
<td>__flush_tlb()</td>
<td>Flushes all TLB entries of the non-global pages owned by the current process. <br>Rewrites cr3 register back into itself flush_tlb,</td>
<td>flush_tlb_mm, flush_tlb_range</td>
</tr>
<tr>
<td>__flush_tlb_global()</td>
<td>Flushes all TLB entries (including those that refer to global pages, that is, pages whose Global flag is set). <br>Disables global pages by clearing the PGE flag of cr4, rewrites cr3 register back into itself, and sets again the PGE flag</td>
<td>flush_tlb_all, flush_tlb_kernel_range</td>
</tr>
<tr>
<td>__flush_tlb_single(addr)</td>
<td>Flushes the TLB of a single Page Table entry of a given process. <br>Executes invlpg assembly language instruction with parameter addr</td>
<td>flush_tlb_page</td>
</tr>
</tbody>
</table>
<p>As a general rule, any process switch implies changing the set of active page tables.
Local TLB entries relative to the old page tables must be flushed.</p>
<p>when the kernel assigns a page frame to a User Mode process and stores its physical address into a Page Table entry, it must flush any local TLB entry that refers to the corresponding linear address.</p>
<ul>
<li>On multiprocessor systems, the kernel also must flush the same TLB entry on the CPUs that are
using the same set of page tables, if any.</li>
<li><em>Lazy</em> TLB mode: if several CPUs are using the same page tables and a TLB entry must be flushed on all of them, then TLB flushing be delayed on CPUs running kernel threads. It is because if the processor switches to a different user process after kernel processing, TLB will be flushed anyway.</li>
</ul>
<h2 id="tlb-shootdowns">TLB shootdowns</h2>
<p>Most processors do not provide coherence guarantees for TLB mappings. Instead the kernel provides this guarantee using a mechanism called a <em>TLB shootdown</em>. It operates by sending <strong>inter-processor interrupts (IPIs)</strong> that runs kernel code to invalidate the stale TLB entries.</p>
<ul>
<li>TLB shootdowns cause each affected core to context switch into the kernel and thus causes latency spikes for the process running on the affected cores.</li>
<li>It will also cause TLB misses when an address with an invalidated page table entry is subsequently accessed.</li>
</ul>
<p>Cause:</p>
<ul>
<li>Every thread in a process shares the same  mm_struct memory descriptor, so calls to <code>munmap()</code> or <code>mremap()</code> in one thread cause TLB shootdowns to occur in the threads running on other cores.</li>
<li>Any operation that narrows a process’ access to memory like <code>munmap()</code> and <code>mprotect()</code> will cause a TLB shootdown.</li>
<li>Calls to the C standard library allocator (<code>malloc</code>, <code>free</code>, etc) will call <code>madvise(...MADV_FREE)</code>/<code>munmap</code> internally, but not necessarily on each invocation.</li>
<li>Other causes of TLB shootdowns are: transparent huge pages (THP), memory compaction, kernel samepage merging (KSM), automatic NUMA memory balancing, page migration and page cache writeback. Details refer to <a href="https://rigtorp.se/virtual-memory/">https://rigtorp.se/virtual-memory/</a></li>
</ul>
<p>Cost:<br>
A TLB shootdown is an IPI so it has the usual interrupt overhead (1us or so).</p>
<ul>
<li>
<p>Benchmarks of an 8-core Broadwell (E5-2667 v4) found that TLB shootdowns of a single page cause interruptions of 598±6 ns (α=0.05) on the 3.10.10x &quot;production&quot; kernel and 941±13 ns (α=0.05) on the 3.10.10x &quot;production-nohz&quot; kernel.</p>
</li>
<li>
<p>TLB shootdowns on 16384 pages cause interruptions of 584±16 ns (α=0.05) on the 3.10.10x &quot;production&quot; kernel and 985±25 ns (α=0.05) on the 3.10.10x &quot;production-nohz&quot; kernel.</p>
</li>
</ul>
<p>The difference in interruption time is not proportional to the number of invalidated pages because Linux dynamically selects the most efficient method of invalidating the affected TLB entry.  However, invalidating a large number of pages forces Linux to flush the entire TLB, which will result in additional performance effects after the TLB shootdown due to the increased rate of page table walks.</p>
<p>Remediation:</p>
<ul>
<li>To avoid TLB shootdowns you can map all needed memory at program startup and avoid calling any functions that modifies the page table after that.</li>
</ul>
<h2 id="hugepage-tlb"><a href="hugepage.html#huge-page-tlb">Hugepage TLB</a></h2>
<br>
<br>
<h1 id="page-cache">Page cache</h1>
<p>The page cache, as its name suggests, is a cache of pages in RAM. The pages originate from reads and writes of regular filesystem files, block device files, and memory-mapped files.</p>
<p>In most cases, the kernel refers to the page cache when reading from or writing to disk. The pages included in the page cache can be of the following types:</p>
<ul>
<li>Pages containing data of regular files.</li>
<li>Pages containing directories.</li>
<li>Pages containing data directly read from block device files (skipping the filesystem layer).</li>
<li>Pages containing data of User Mode processes that have been swapped out on disk.</li>
<li>Pages belonging to files of special filesystems, such as shm and shared memory region.</li>
</ul>
<p>Practically all <code>read()</code> and <code>write()</code> file operations rely on the page cache.</p>
<ul>
<li>unless a file is opened with the O_DIRECT flag. In this case, the page cache is bypassed and the I/O data transfers make use of user-space buffers.</li>
</ul>
<p>Under Linux, the number of megabytes of main memory currently used for the page cache is indicated in the <code>buff/cache</code> column of the report produced by the <code>free -m</code> command.</p>
<p><img src="images/2022-02-27-10-27-31.png" alt=""></p>
<p>The core data structure of the page cache is the <code>address_space</code> object, refer to <a href="#address_space-object">section here</a></p>
<h2 id="buffer-cache">Buffer Cache</h2>
<p>Individual disk blocks also tie into the page cache, by way of block I/O buffers. A buffer is the in-memory representation of a single physical disk block. Buffers act as descriptors that map pages in memory to disk blocks.</p>
<p>The <a href="files.html#kernel-buffer-cache">page cache</a> reduces disk access during block I/O operations by both caching disk blocks and buffering block I/O operations until later. This caching is often referred to as the <em>buffer cache</em>, although as implemented it is not a separate cache but is part of the page cache.</p>
<ul>
<li>If cached data has both a file and a block representation—as most data does—the buffer cache will simply point into the page cache; thus only one instance of the data is cached in memory.</li>
<li>The buffer and page caches were not always unified, until kernel v2.4.</li>
</ul>
<h2 id="dirty-page-cache-write">Dirty Page Cache Write</h2>
<p>Write operations are deferred in the page cache. Dirty page writeback occurs in below three situations:</p>
<ul>
<li>
<p>When free memory shrinks below a specified threshold, the kernel writes dirty data
back to disk to free memory because only clean (nondirty) memory is available for
eviction.</p>
<ul>
<li><strong><code>dirty_background_ratio</code></strong>: Percentage (default 10%) of dirty system memory to trigger pdflush background write-back, the kernel invokes the <code>wakeup_flusher_threads()</code> call to wake up one or more flusher threads and have them run the <code>bdi_writeback_all()</code> function to begin writeback of dirty pages.</li>
<li><strong><code>dirty_background_bytes</code></strong>: introduced in kernel v2.6.29, amount of dirty memory to trigger pdflush
background write-back. If both <code>dirty_background_ratio</code> and <code>dirty_background_bytes</code> are set, &quot;bytes&quot; parameter takes precedence.</li>
<li><strong><code>dirty_ratio</code></strong> (default 20%）and <strong><code>dirty_bytes</code></strong>: Percentage or amount of dirty memory that causes a writing process to block to handle dirty page write-back first.</li>
</ul>
</li>
<li>
<p>When dirty data grows older than a specific threshold, sufficiently old data is written
back to disk to ensure that dirty data does not remain dirty indefinitely.</p>
<ul>
<li>a timer is set every <strong><code>dirty_writeback_centisecs</code></strong> 0.01 seconds (default 500, 5 secs), it wakes up a flusher thread and have it run the <code>wb_writeback()</code> function. This function then writes back all data that was modified longer than <strong><code>dirty_expire_centisecs</code></strong> 0.01 seconds (default 3000, 30 secs) ago.</li>
<li>要想知道page的最近更新时间，最简单的方法当然是每个page维护一个timestamp，但这个开销太大了，而且全部扫描一次也会非常耗时，因此具体实现中不会以page为粒度，而是按照inode中记录的dirtying-time来算。</li>
</ul>
</li>
<li>
<p>When a user process invokes the <code>sync()</code> and <code>fsync()</code> system calls, the kernel performs writeback on demand.</p>
</li>
</ul>
<p>The system administrator can set these values either in <strong><code>/proc/sys/vm</code></strong> or via sysctl.</p>
<blockquote>
<p>don't confuse page write-back (with pdflush, used to sync data with storage medium) and page reclaim (with kswapd, used to free up pages when free memory is low)</p>
</blockquote>
<h3 id="pdflush-kernel-thread">pdflush kernel thread</h3>
<p>since kernel v2.6, the kernel threads <strong><code>pdflush threads</code></strong>, performs all three jobs.</p>
<p>there was a pool of pdflush threads, between two and eight as needed. 如果1秒内都没有空闲的pdflush线程可用，内核将创建一个新的pdflush线程，反之，如果某个pdflush线程的空闲时间已经超过1秒，则该线程将被销毁。一个块设备可能有多个可以传输数据的队列，为了避免在队列上的拥塞（congestion），pdflush线程会动态的选择系统中相对空闲的队列。</p>
<p>Since Kernel v2.6.32, pdflush threads have since been replaced by the <strong><code>flusher threads</code></strong> (named flush), which are created per device to better balance the per-device workload and improve throughput.</p>
<ul>
<li>The <code>flusher</code> code lives in <code>mm/page-writeback.c</code> and <code>mm/backing-dev.c</code> and the writeback mechanism lives in <code>fs/fs-writeback.c</code>.</li>
</ul>
<p>无论是内核周期性扫描，还是用户手动触发，flusher threads的writeback都是间隔一段时间才进行的，如果在这段时间内系统掉电了（power failure），那还没来得及writeback的数据修改就面临丢失的风险，这是page cache机制存在的一个缺点。writeback越频繁，数据因意外丢失的风险越低，但同时I/O压力也越大。</p>
<h3 id="write-back-writable-mmapped-file">write-back writable mmapped file</h3>
<p>If any of the dirty pages are part of a writable memory mapping, the writeback process must first update the page table to mark the page as read-only before writing it to disk. Any subsequent memory write to the page will cause a page fault, letting the kernel update the page cache state to dirty and mark the page writable again. In practice this means that writeback causes TLB shootdowns and that writes to pages that are currently being written to disk must stall until the disk write is complete. This leads to latency spikes for any process that is using file backed writable memory mappings.</p>
<p>Remediation:<br>
To avoid latency spikes due to page cache writeback you cannot create any file backed (or more precisely page cache backed) writable memory mappings. Creating anonymous writable memory mappings using <code>mmap(MAP_ANONYMOUS)</code> or by mapping files on Linux <code>tmpfs</code> or <code>hugetlbfs</code> filesystem is fine.</p>
<br>
<h1 id="cache-warm">Cache Warm</h1>
<p>_mm_prefetch
Intel intrinsic: <a href="https://www.felixcloutier.com/x86/prefetchh">https://www.felixcloutier.com/x86/prefetchh</a></p>
<ul>
<li>It fetches <strong>one cache line of data</strong></li>
</ul>
<br>
Another way:<br>
Memcpy the function ptr and its length to a buffer (can be just a static buffer)
<p>How to get function length?</p>
<ul>
<li>
<p>One way to determine the size of a function. The command is:</p>
<pre><code class="language-bash">nm -S &lt;object_file_name&gt;
</code></pre>
<p>This will return the sizes of each function inside the object file. Consult the manual pages in the GNU using 'man nm' to gather more information on this.</p>
</li>
<li>
<p>another way is to parse object file, use elf parser</p>
<ul>
<li>Go to symbol table section, get function object, there is symbol value (function name), and size (function size)</li>
<li>In code, demangle the function name to find the function name from elf parsing</li>
</ul>
</li>
</ul>
<h3 id="hardware-prefetch"><a href="memory.html#hardware-prefetch">hardware prefetch</a></h3>
<br>
<h1 id="cat---cache-allocation-techonology">CAT - Cache allocation techonology</h1>
<h2 id="inclusive-l3">Inclusive L3</h2>
<p>In Xeon the cache consists of 3 levels - L1, L2 and The Last Level Cache (aka LLC or L-3). L1 and L2 caches are assigned to cores exclusively, while LLC is shared among multiple cores. In other words, only the assigned core can read or modify the content L1 and L2 cache lines (line is a smallest unit the cache is transported. Its size is fixed at 64 bytes for all Xeon CPU). Attaching a cache to a CPU contributes to increasing performance - it takes 4 cycles to load a data from L1 cache, 12 cylcles from L2, 36-31 cycles from the Last Level Cache (190 from local memory, 310 from remote)</p>
<p><img src="images/2022-02-13-14-16-22.png" alt=""></p>
<p>The cache on Xeon processors before Skylake (Sandy, Ivy, Haswell and Broadwell) is inclusive. It means that once a data is read from memory it fills all levels of cache and cache will include the data from the lower levels. LLC will include a copy of L2 and L1 contents. L2 will include a copy of L1. Once the data in L1 or L2 is modified, the change has to be propagated to LLC, to ensure the &quot;coherency&quot; of its contents. Each cache will listen (&quot;snoop&quot;) to cache change messages and act accordingly.</p>
<h3 id="skylake-change">Skylake change</h3>
<ol>
<li>Shift cache balance from shared-distributed to private-local by enlarging L2.</li>
<li>LLC is converted to non-inclusive, so when one core evicts a line from L3 that is cached in L1/L2 of another core it does not mean the L1/L2 caches will be evicted as well. As discussed in the performance section, it leads to the lower ratio of cache misses even when no cache allocation technology is employed</li>
</ol>
<p><img src="images/2022-02-13-14-19-39.png" alt=""></p>
<h2 id="introduction-to-cache-allocation-technology-cat">Introduction to Cache Allocation Technology (CAT)</h2>
<p>Cache Allocation Technology (CAT) is a feature on  Broadwell and newer CPU families, enabling controls for allocating and isolating L3 cache segments to particular cores.</p>
<p>On Broadwell CPUs, the L3 cache is shared by all cores across the same socket.  This means that one core can evict cache lines which are in use by other cores. The cache on Broadwell is &quot;inclusive&quot;, meaning that a copy of higher level cache has to reside also in the lower level. For example, a copy of L2 has to reside in L3. If a cache line is evicted from L3, to comply with that cache coherency requirement the corresponding line will also be evicted from L2. Even if cores are dedicated to the same process, the isolation is not perfect. Given the increase in core count on single processes, this can have negative impacts to performance when there are cache misses. Some &quot;noisy neighbors&quot; (e.g. kernel, <strong>market data</strong> or <strong>slow path threads that may use a lot of cache</strong>), <strong>can severely impact the performance of fast path threads</strong>. Intel has attempted to fix this by introducing CAT. CAT solves this by essentially slicing the L3 into discrete blocks of L3. These blocks are then assigned per core, trading off max utilization of L3 for better individual core performance.</p>
<p>CAT allows the system to split the L3 into units of cache called 'ways'. A way is a unit of cache (In Broadwell, 20 ways available regardless of L3 size) that can be assigned to a core. Each core can then receive a policy allowing it to utilize one or more of these 1/20th fractions of the total L3 cache.</p>
<p><img src="images/2022-02-13-14-21-22.png" alt=""></p>
<p>In contrast to Broadwell, Skylake has only 11 cache ways, although the size of the cache is the same 20M. Consequently each cache way is larger.</p>
<p>A variant of this configuration system is called CDP, which allows further fine tuning of cache allocations by splitting the L3 into a data and code segment. So far there is no data to indicate that using CDP is advantageous for lower latency than CAT provides alone.</p>
<p>For more info see</p>
<ul>
<li><a href="https://software.intel.com/en-us/articles/introduction-to-cache-allocation-technology">https://software.intel.com/en-us/articles/introduction-to-cache-allocation-technology</a></li>
<li><a href="http://www.intel.nl/content/dam/www/public/us/en/documents/white-papers/cache-allocation-technology-white-paper.pdf">http://www.intel.nl/content/dam/www/public/us/en/documents/white-papers/cache-allocation-technology-white-paper.pdf</a></li>
<li><a href="https://software.intel.com/en-us/articles/code-and-data-prioritization-proof-points-and-software-enabling">https://software.intel.com/en-us/articles/code-and-data-prioritization-proof-points-and-software-enabling</a></li>
</ul>
<br>
<h1 id="ddio">DDIO</h1>
<p>DDIO (Data Direct I/O) is a hardware feature found in recent Intel Xeon server processors that allows network devices and other peripherals to directly access the CPU's last-level cache (LLC) instead of going through main memory.</p>
<ul>
<li>DDIO is enabled by default on Intel Xeon E5, E7 and newer Scalable Processor families like Cascade Lake and Ice Lake.</li>
<li>By default, DDIO uses 2 ways out of the total LLC ways on Intel Xeon processors. For example, on an 11-way LLC, DDIO uses 2 out of the 11 ways. In practice, DDIO uses the two rightmost ways in LLC(i.e., bitmask of 0x003). The specific LLC ways used by DDIO are statically assigned and cannot be changed by software.</li>
<li>DDIO's cache region is not isolated from CPU cores or other applications. This can lead to cache contention if the CPU cores also use the same ways as DDIO.</li>
<li>DDIO is ineffective for the remote socket and it pollutes the LLC on the socket connected to the NIC.</li>
<li>For Cascade Lake (2nd Gen Scalable) processors, you can enable/disable DDIO on a per PCIe root port basis</li>
</ul>
<p><img src="images/2024-05-04-09-51-24.png" alt=""></p>
<p>figure b: DCA exploits PCIe Transaction Layer Packet Processing Hint,making it possible to prefetch portions of I/Odata to the processor’s cache. Potentially,this overcomes the drawbacks of traditional DMA, thereby achieving maximal I/O bandwidth and reducing processor stall time. it is still inefficient in terms of memory bandwidth usage since the whole packet is DMAed into main memory.</p>
<p><strong>Writing packets</strong>. When a NIC writes a cacheline to LLC via PCIe, DDIO overwrites the cache line if it is already present in any LLC way(aka a PCIe write hit or write update). Otherwise,the cacheline is allocated in the LLC and DDIO writes the data in to the newly allocated cacheline (aka a PCIe write miss or write allocate). In the latter case, DDIO is restricted to use only a limited portion of LLC when allocating cache lines. It is possible to artificially increase this portion by warming up the cache with processor writes to the address of these buffers, then DDIO performs write-updates.</p>
<p><strong>Reading packets.</strong> A NIC can read a cache line from LLC if the cache line is present in any LLC way (aka a PCIe read hit). Otherwise, the NIC reads a cache-line-sized chunk from system memory (aka a PCIe read miss).</p>
<p>reference:</p>
<ul>
<li><a href="https://www.usenix.org/system/files/atc20-farshin.pdf">https://www.usenix.org/system/files/atc20-farshin.pdf</a></li>
<li><a href="https://www.youtube.com/watch?v=m_yx_Sbao2I&amp;ab_channel=USENIX">https://www.youtube.com/watch?v=m_yx_Sbao2I&amp;ab_channel=USENIX</a></li>
</ul>
<br>
<h1 id="kernel-internals">Kernel Internals</h1>
<h2 id="address_space-object">address_space object.</h2>
<p>To maintain a generic page cache, one not tied to physical files or the inode structure, Linux uses <code>address_space</code> object to represent a page cache. Think of <code>address_space</code> as the physical analogue to the virtual <code>vm_area_struct</code>.</p>
<p>一个address_space管理了一个文件在内存中缓存的所有pages。</p>
<ul>
<li>mmap映射可以将文件的一部分区域映射到虚拟地址空间的一个VMA，如果有5个进程，每个进程mmap同一个文件两次（文件的两个不同部分），那么就有10个VMAs，但address_space只有一个。</li>
<li>每个进程打开一个文件的时候，都会生成一个表示这个文件的struct file，但是文件的struct inode只有一个，inode才是文件的唯一标识，指向address_space的指针就是内嵌在inode结构体中的。</li>
<li>在page cache中，每个page都有对应的文件，这个文件就是这个page的owner，address_space将属于同一owner的pages联系起来.</li>
</ul>
<p>The <code>address_space</code> structure is defined in <code>&lt;linux/fs.h&gt;</code>:</p>
<pre><code class="language-C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">address_space</span> {
    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">inode</span> *host;                 <span class="hljs-comment">/* owning inode */</span>
    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">radix_tree_root</span> page_tree;   <span class="hljs-comment">/* radix tree of all pages */</span>
    <span class="hljs-type">spinlock_t</span> tree_lock;               <span class="hljs-comment">/* page_tree lock */</span>
    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> i_mmap_writable;       <span class="hljs-comment">/* VM_SHARED ma count */</span>
    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">prio_tree_root</span> i_mmap;       <span class="hljs-comment">/* list of all mappings */</span>
    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">list_head</span> i_mmap_nonlinear;  <span class="hljs-comment">/* VM_NONLINEAR ma list */</span>
    <span class="hljs-type">spinlock_t</span> i_mmap_lock;             <span class="hljs-comment">/* i_mmap lock */</span>
    <span class="hljs-type">atomic_t</span> truncate_count;            <span class="hljs-comment">/* truncate re count */</span>
    <span class="hljs-type">unsigned</span> <span class="hljs-type">long</span> nrpages;              <span class="hljs-comment">/* total number of pages */</span>
    <span class="hljs-type">pgoff_t</span> writeback_index;            <span class="hljs-comment">/* writeback start offset */</span>
    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">address_space_operations</span> *a_ops; <span class="hljs-comment">/* operations table */</span>
    <span class="hljs-type">unsigned</span> <span class="hljs-type">long</span> flags;                <span class="hljs-comment">/* gfp_mask and error flags */</span>
    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">backing_dev_info</span> *backing_dev_info; <span class="hljs-comment">/* read-ahead information */</span>
    <span class="hljs-type">spinlock_t</span> private_lock;            <span class="hljs-comment">/* private lock */</span>
    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">list_head</span> private_list;      <span class="hljs-comment">/* private list */</span>
    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">address_space</span> *assoc_mapping; <span class="hljs-comment">/* associated buffers */</span>
};
</code></pre>
<ul>
<li>The <code>i_mmap</code> field is a priority search tree of all shared and private mappings in this address space.
<ul>
<li>A priority search tree is a clever mix of heaps and radix trees, defined in <code>&lt;linux/radix-tree.h&gt;</code></li>
<li>one address_space structure, it can have many vm_area_struct structures—a one-to-many mapping from the physical pages to many virtual pages.</li>
<li>i_mmap field allows the kernel to efficiently find the mappings associated with this cached file.</li>
</ul>
</li>
<li>a total of <code>nrpages</code> in the address space</li>
<li>The address_space is normally associated with an inode, if so, the <code>host</code> field points to the inode. If it is NULL, there is no associated indoe (eg. it can associated with the swapper)</li>
<li>The <code>a_ops</code> field points to the address space operations table.</li>
</ul>
<h3 id="radix-tree-pages">Radix tree pages</h3>
<p>Radix tree的每个节点可以存放64个slots（由<code>RADIX_TREE_MAP_SHIFT</code>设定，小型系统为了节省内存可以配置为16），每个slot的指针指向下一层节点，最后一层slot的指针指向<code>struct page</code>，因此一个高度为2的radix tree可以容纳64个pages，高度为3则可以容纳4096个pages。</p>
<p>如何在radix tree中找到一个指定的page呢？那就要回顾下<code>struct page</code>中的<code>mapping</code>和<code>index</code>域了，<code>mapping</code>指向page所属文件对应的<code>address_space</code>，进而可以找到<code>address_space</code>的radix tree，<code>index</code>既是page在文件内的offset，也可作为查找这个radix tree的索引，因为radix tree就是按page的<code>index</code>来组织<code>struct page</code>的。</p>
<p>这里是用page index中的一部分bits作为radix tree第一层的索引，另一部分bits作为第二层的索引，以此类推。因为一个radix tree节点存放64个slots，因此一层索引需要6个bits，如果radix tree高度为2，则需要12个bits。</p>
<p>内核中具体的查找函数是<code>find_get_page(mapping, offset)</code>，如果在page cache中没有找到，就会触发page fault，调用<code>__page_cache_alloc()</code>在内存中分配若干物理页面，然后将数据从磁盘对应位置copy过来，通过<code>add_to_page_cache()--&gt;radix_tree_insert()</code>放入radix tree中。在将一个page添加到page cache和从page cache移除时，需要将page和对应的radix tree都上锁。</p>
<p><img src="images/2022-02-26-22-01-27.png" alt=""></p>
<p>Linux中radix tree的每个slot除了存放指针，还存放着标志page和磁盘文件同步状态的tag。如果page cache中一个page在内存中被修改后没有同步到磁盘，就说这个page是dirty的，此时tag就是<code>PAGECACHE_TAG_DIRTY</code>。如果正在同步，tag就是<code>PAGECACHE_TAG_WRITEBACK</code>。</p>
<p>只要下一层中有一个slot指向的page是dirty的，那么上一层的这个slot的tag就是&quot;dirty&quot;的。</p>
<p>前面介绍struct page中的flags时提到，flags可以是<code>PG_dirty</code>或<code>PG_writeback</code>，既然struct page中已经有了标识同步状态的信息，为什么这里radix tree还要再加上tag来标记呢？</p>
<p>因为page数量众多，内核不可能为每一个page维护一个timer，因此在判断是否应该writeback时，是以inode为单位的，而一个inode对应的address space中，如果去逐一比对每个page的&quot;<code>PG_dirty</code>&quot;标志位，将做很多无用功。而当slot也加上&quot;dirty&quot;标志位后，那么如果slot是clean的，就没有必要再扫描其下一层的slot和page了，这样可以减少开销。</p>
<p>现在address_space中radix tree已经被<strong>xarray</strong>取代了（<a href="https://lwn.net/Articles/745073/">参考这篇文章</a>）。</p>
<h3 id="find-ptes-using-priority-search-tree-mmap">Find PTEs using priority search tree (mmap)</h3>
<p>如果要回收page cache中一个页面，回收之前，需要找到所有指向这个page的PTE页表项。需要将这些PTE中P标志位设为0（not present），同时将page的物理页面号PFN也全部设成0，要不然下次PTE指向的位置存放的就是无效的数据了。</p>
<p>从虚拟地址映射到物理地址是正向映射，而通过物理页面寻找映射它的虚拟地址，则是reverse mapping（逆向映射）。page的确没有直接指向PTE的反向指针，但是通过page在文件中的offset/index和address_space的VMA mmap线性映射，就可以知道VMA中的哪个虚拟地址映射了这个page。</p>
<p><img src="images/2022-02-26-22-23-42.png" alt=""></p>
<p><img src="images/2022-02-26-22-40-22.png" alt=""></p>
<p>PST是一种糅合了radix tree和heap的数据结构，具体实现较为复杂，现在已经被基于augmented rbtree的interval tree所取代，详情请参考<a href="https://lwn.net/Articles/509994/">这篇文章</a>。</p>
<h3 id="address_space-operations">address_space operations</h3>
<p>The operations table is represented by <code>struct address_space_operations</code> and is also defined in <code>&lt;linux/fs.h&gt;</code>:</p>
<pre><code class="language-C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">address_space_operations</span> {
    <span class="hljs-built_in">int</span> (*writepage)(<span class="hljs-keyword">struct</span> page *, <span class="hljs-keyword">struct</span> writeback_control *);
    <span class="hljs-built_in">int</span> (*readpage) (<span class="hljs-keyword">struct</span> file *, <span class="hljs-keyword">struct</span> page *);
    <span class="hljs-built_in">int</span> (*sync_page) (<span class="hljs-keyword">struct</span> page *);
    <span class="hljs-built_in">int</span> (*writepages) (<span class="hljs-keyword">struct</span> address_space *, <span class="hljs-keyword">struct</span> writeback_control *);
    <span class="hljs-built_in">int</span> (*set_page_dirty) (<span class="hljs-keyword">struct</span> page *);
    <span class="hljs-built_in">int</span> (*readpages) (<span class="hljs-keyword">struct</span> file *, <span class="hljs-keyword">struct</span> address_space *, <span class="hljs-keyword">struct</span> list_head *, <span class="hljs-type">unsigned</span>);
    <span class="hljs-built_in">int</span> (*write_begin)(<span class="hljs-keyword">struct</span> file *, <span class="hljs-keyword">struct</span> address_space *mapping,
                        <span class="hljs-type">loff_t</span> pos, <span class="hljs-type">unsigned</span> len, <span class="hljs-type">unsigned</span> flags,
                        <span class="hljs-keyword">struct</span> page **pagep, <span class="hljs-type">void</span> **fsdata);
    <span class="hljs-comment">// ... more operations omitted ...</span>
};
</code></pre>
<p>Each backing store implements how it interacts with the page cache via its own address_space_operations. For example, the ext3 filesystem defines its operations in <code>fs/ext3/inode.c</code>.</p>
<pre><code class="language-C++"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">address_space_operations</span> ext3_writeback_aops = {         
    .readpage       = ext3_readpage,                 
    .writepage      = ext3_writeback_writepage,
    .releasepage    = ext3_releasepage,
    ...
}
</code></pre>

            <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
            
        </body>
        </html>