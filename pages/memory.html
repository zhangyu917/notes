<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Memory</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        <style>
#button {
    display: inline-block;
    background-color: #FF9800;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    /*visibility: hidden;*/
    z-index: 1000;
}
#button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
}
#button:hover {
    cursor: pointer;
    background-color: #333;
}
#button:active {
    background-color: #555;
}
#button.show {
    opacity: 1;
    visibility: visible;
}

#btn-back-to-top {
    position: fixed;
    bottom: 20px;
    right: 20px;
    display: none;
  }

  .to-top {
    background: white;
    position: fixed;
    bottom: 16px;
    right:32px;
    width:50px;
    height:50px;
    border-radius: 50%;
    border-color: white;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size:32px;
    color:#1f1f1f;
    text-decoration: none;
    opacity: 0.5;
    pointer-events: auto;
    transition: all .4s;
    transform: rotate(270deg);
  }
  
</style>
    </head>
    <body class="vscode-body vscode-light">
        <ul>
<li><a href="#memory">Memory</a>
<ul>
<li><a href="#typical-latency-of-a-memory-subsystem">Typical latency of a memory subsystem.</a></li>
<li><a href="#main-memory--ddr">Main Memory &amp; DDR</a></li>
<li><a href="#hardware-prefetch">Hardware Prefetch</a></li>
</ul>
</li>
<li><a href="#profile-memory-performance">Profile memory performance</a></li>
<li><a href="#optimizing-memory-access">Optimizing memory access</a>
<ul>
<li><a href="#general-tips">General tips</a></li>
<li><a href="#cache-ways-and-critical-stride">Cache ways and Critical stride</a></li>
<li><a href="#keep-related-functions-together">Keep related functions together</a></li>
<li><a href="#alignment">Alignment</a></li>
<li><a href="#use-efficient-data-structures">Use efficient data structures</a>
<ul>
<li><a href="#use-stdvector-over-stdlist">Use std::vector over std::list</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#explicit-cache-control-instruction">Explicit cache control instruction</a>
<ul>
<li><a href="#prefetch-instruction">Prefetch instruction</a></li>
<li><a href="#word-of-caution">Word of caution</a></li>
<li><a href="#nontemporal-cache-writes">Nontemporal cache writes</a></li>
</ul>
</li>
<li><a href="#measuring-memory-and-cache-speed">Measuring memory and cache speed</a>
<ul>
<li><a href="#sequentially-accessing-reading-and-writing-a-large-array">Sequentially accessing (reading and writing) a large array</a></li>
<li><a href="#random-access-an-array">Random access an array</a></li>
<li><a href="#random-access-read-result">Random access (read) result</a></li>
<li><a href="#random-access-write-result">Random access (write) result</a></li>
<li><a href="#sequential-access-write-result">Sequential access (write) result</a></li>
</ul>
</li>
</ul>
<h1 id="memory">Memory</h1>
<p>Note that waiting on memory does not count as waiting in this sense: when a thread is waiting on memory, it just takes longer to execute one instruction. When a thread is waiting on I/O, it has to make an operating system call, then it's blocked by the OS and isn't executing anything at all until the OS wakes it up to process the data.</p>
<h2 id="typical-latency-of-a-memory-subsystem">Typical latency of a memory subsystem.</h2>
<table>
<thead>
<tr>
<th>Memory Hierarchy Component</th>
<th>Latency (cycle/time)</th>
</tr>
</thead>
<tbody>
<tr>
<td>L1 Cache</td>
<td>4 cycles (~1 ns)</td>
</tr>
<tr>
<td>L2 Cache</td>
<td>10-25 cycles (5-10 ns)</td>
</tr>
<tr>
<td>L3 Cache</td>
<td>~40 cycles (20 ns)</td>
</tr>
<tr>
<td>Main Memory</td>
<td>200+ cycles (100 ns)</td>
</tr>
</tbody>
</table>
<p>an instruction (I-cache) cache miss is characterized as a Front-End stall</p>
<ul>
<li>I-cache miss happens during instruction fetch, it is attributed as a Front-End issue.
a data cache (D-cache) miss is characterized as a Back-End stall.</li>
<li>when the data requested by this load is not found in the D-cache, this will be categorized as a Back-End issue.</li>
</ul>
<h2 id="main-memory--ddr">Main Memory &amp; DDR</h2>
<p>DDR (double data rate) DRAM technology is the predominant DRAM technology.</p>
<ul>
<li>DRAM bandwidths have improved every generation while the DRAM latencies have stayed the same or even increased.</li>
<li>The latencies shown in this table correspond to the latency in the DRAM device itself. the latencies as seen from the CPU pipeline (cache miss on a load to use) are higher (in the 70ns-150ns range) due to additional latencies and queuing delays incurred in the cache controllers, memory controllers, and on-die interconnects.</li>
<li>Modern CPUs support multiple, independent channels of DDR DRAM memory.</li>
</ul>
<table>
<thead>
<tr>
<th>DDR Generation</th>
<th>Highest Data Rate (million transfers/s)</th>
<th>Typical Read Latency (ns)</th>
</tr>
</thead>
<tbody>
<tr>
<td>DDR3</td>
<td>2133</td>
<td>10.3</td>
</tr>
<tr>
<td>DDR4</td>
<td>3200</td>
<td>12.5</td>
</tr>
<tr>
<td>DDR5</td>
<td>6400</td>
<td>14</td>
</tr>
</tbody>
</table>
<h2 id="hardware-prefetch">Hardware Prefetch</h2>
<p>When accessing memory sequentially, the hardware can begin transferring the next element of the array right away: the very first element still takes 7 nanoseconds to access, but after that, the hardware can start streaming the entire array from or to memory as fast as the CPU and the memory bus can handle it. Thus, the latency is no longer the limiting factor, the bandwidth is.</p>
<p>In general:</p>
<ul>
<li>prefetch is equally effective for accessing memory in increasing and decreasing orders.
<ul>
<li>However, reversing the direction will incur some penalty until the prefetch adjusts to the new pattern.</li>
</ul>
</li>
<li>Accessing memory with stride, such as accessing every fourth element in an array, will be detected and predicted just as efficiently as dense sequential access.</li>
<li>The prefetch can detect multiple concurrent strides (that is, accessing every third and every seventh element)</li>
</ul>
<p><strong>Guideline</strong>: Prefer regular predictable  memory access patterns</p>
<br>
<h1 id="profile-memory-performance">Profile memory performance</h1>
<p>Using perf to see the overall memory usage</p>
<p><code>$ perf stat -e cycles,instructions,L1-dcache-load-misses,L1-dcache-loads  ./program</code></p>
<br>
<h1 id="optimizing-memory-access">Optimizing memory access</h1>
<h2 id="general-tips">General tips</h2>
<p>If recomputing the value takes less than memory access, recomputing some values that could have been stored and retrieved from memory</p>
<p>Try to fit the current working data set into one of the caches, say, the L2 cache, and do as much work on it as possible before moving to the next section of the data.</p>
<h2 id="cache-ways-and-critical-stride">Cache ways and Critical stride</h2>
<p>Variables whose distance in memory is a multiple of the critical stride will contend for the same cache lines.</p>
<p>The critical stride can be calculated as:</p>
<p><code>critical stride = (number of sets) * (cahce line size) = (total cache size) / (number of ways).</code></p>
<p>E.g.  8KB cache with 4 ways, the critical stride is 2048 bytes (0x800)</p>
<h2 id="keep-related-functions-together">Keep related functions together</h2>
<p>The code cache works most efficiently if functions that are used near each other are also stored near each other in the code memory.</p>
<p>To keep the two modules contiguous in program memory, we can control the order in which the modules are linked together. The link order is usually the order in which the modules appear in the project window or makefile.</p>
<p>You can check the order of functions in memory by requesting a map file from the linker. The map file tells the address of each function relative to the beginning of the program.</p>
<ul>
<li>The map file includes the addresses of library functions linked from static libraries (.lib or .a), but not dynamic libraries (.dll or .so).</li>
</ul>
<h2 id="alignment">Alignment</h2>
<p>A variable is accessed most efficiently if it is stored at a memory address which is divisible by the size of the variable.</p>
<ul>
<li>You can generally assume that the compiler takes care of this alignment automatically.</li>
<li>The alignment of structure and class members may cause a waste of cache space</li>
</ul>
<p>You may choose to align large objects and arrays by the cache line size</p>
<pre><code class="language-C++"><span class="hljs-built_in">alignas</span>(<span class="hljs-number">64</span>) <span class="hljs-type">int</span> BigArray[<span class="hljs-number">1024</span>];
</code></pre>
<h2 id="use-efficient-data-structures">Use efficient data structures</h2>
<h3 id="use-stdvector-over-stdlist">Use std::vector over std::list</h3>
<p>What if we dont know the number of items, and growing a vector is extremely inefficient because of the copying involved.</p>
<ul>
<li>We can use a block-allocated array</li>
<li>It allocates memory in blocks of a fixed amount, usually small enough that they fit into the L1 cache (anywhere between 2 KB and 16 KB is commonly used). If we need grow, we just allocate a new block.</li>
<li>Each block is used as an array. Only accessing the first element of each block is likely to incur a cache miss.</li>
<li>It is similar to std::deque (unfortunately, the implementation in most STL versions is not particularly efficient, and sequential accesses to the deque are usually somewhat slower than to the vector of the same size)</li>
</ul>
<p>What if we need insertion?</p>
<ul>
<li>Depend on the data usage. If the access pattern changes in time, the data structure should change as well to make sure that particular access pattern is optimal.</li>
<li>If we need sorted order, but not very often, it maybe advantageous to separate the ordering from the storage. The data is stored in a vector or a deque, and the order is imposed on top of it by an array of pointers sorted in the desired order.</li>
</ul>
<br>
<h1 id="explicit-cache-control-instruction">Explicit cache control instruction</h1>
<h2 id="prefetch-instruction">Prefetch instruction</h2>
<p>The prefetch instruction can be used for fetching a cache line that we expect to use later in
the program flow.</p>
<p><img src="./images/2022-01-24-12-02-03.png" alt=""></p>
<h2 id="word-of-caution">Word of caution</h2>
<p>At least from the author and his tests, these explicit cache control instructions don’t provide performance improvement in general.</p>
<p>The reason is that modern processors prefetch data automatically thanks to out-of-order execution and advanced prediction mechanisms.</p>
<p>Prefetch techniques also need to balance between demand and prefetch requests to guard against prefetch traffic slowing down demand traffic.</p>
<h2 id="nontemporal-cache-writes">Nontemporal cache writes</h2>
<p>Memory writes are more expensive than reads when cache misses occur in a write-back cache. A whole cache line has to be read from memory, modified, and written back in case of a cache miss.</p>
<p>The nontemporal write instructions (MOVNT) write directly to memory without loading a cache line.</p>
<ul>
<li>This could be useful in cases where we are writing to uncached memory and we do not expect to read from the same or a nearby address again before the cache line would be evicted.</li>
<li>Results show nontemporal writes is <strong>only</strong> advantageous if a level-2 cache miss can be expected.
<ul>
<li>If the cache contentions can be prevented in other ways then the nontemporal write instructions are not optimal.</li>
<li>As a rule of thumb, it can be recommended to use non-temporal writes only when writing a memory block that is bigger than half the size of the largest-level cache.</li>
</ul>
</li>
</ul>
<br>
<h1 id="measuring-memory-and-cache-speed">Measuring memory and cache speed</h1>
<h3 id="sequentially-accessing-reading-and-writing-a-large-array">Sequentially accessing (reading and writing) a large array</h3>
<ul>
<li>memory range to read
<ul>
<li>From 1 &lt;&lt; 10 (1K) to 1 &lt;&lt; 30 (1G), doubling every time</li>
</ul>
</li>
<li>Each read size
<ul>
<li>int, long, __m128i, __m256i</li>
</ul>
</li>
<li>Perform several memory read per iteration
<ul>
<li>Using REPEAT macro</li>
</ul>
</li>
</ul>
<h3 id="random-access-an-array">Random access an array</h3>
<p>Precompute the random indices and store them in another array.</p>
<pre><code class="language-C++"><span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; N; ++i) v_index[i] = i;
std::<span class="hljs-built_in">random_shuffle</span>(v_index.<span class="hljs-built_in">begin</span>(), v_index.<span class="hljs-built_in">end</span>());
</code></pre>
<h3 id="random-access-read-result">Random access (read) result</h3>
<p><img src="images/2022-01-24-13-23-59.png" alt=""></p>
<ul>
<li>the L1 cache of 32 KB</li>
<li>The L2 cache is 256 KB.</li>
<li>The L3 cache 8MB</li>
</ul>
<p>Access latency doesn’t depend on the read size. It implies that the read speed is limited by latency and not by bandwidth.</p>
<h3 id="random-access-write-result">Random access (write) result</h3>
<p><img src="images/2022-01-24-13-25-51.png" alt=""></p>
<h3 id="sequential-access-write-result">Sequential access (write) result</h3>
<p><img src="images/2022-01-24-13-26-05.png" alt=""></p>
<ul>
<li>Y-axis scale is smaller than random access pattern, up range dropped from 7ns to 2.5ns</li>
<li>The curves for different word sizes are no longer the same (highly hardware dependent).
<ul>
<li>Same for L1 cache write, but different for other caches and main memory</li>
<li>For main memory numbers, the write times double every time the word size doubles.</li>
<li>This implies that the speed is now limited not by latency but by bandwidth. We're pushing the bits into memory as fast as the bus can transmit them, and it doesn't matter whether we group them into 64-bit chunks or 256-bit chunks that we call words, we've hit the bandwidth limit of the memory.</li>
</ul>
</li>
</ul>

        <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
        
    </body>
    </html>