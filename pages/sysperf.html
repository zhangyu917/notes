<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>System Performance</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        <style>
#button {
    display: inline-block;
    background-color: #FF9800;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    /*visibility: hidden;*/
    z-index: 1000;
}
#button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
}
#button:hover {
    cursor: pointer;
    background-color: #333;
}
#button:active {
    background-color: #555;
}
#button.show {
    opacity: 1;
    visibility: visible;
}

#btn-back-to-top {
    position: fixed;
    bottom: 20px;
    right: 20px;
    display: none;
  }

  .to-top {
    background: white;
    position: fixed;
    bottom: 16px;
    right:32px;
    width:50px;
    height:50px;
    border-radius: 50%;
    border-color: white;
    display: flex;
    align-items: center;
    justify-content: center;
    font-size:32px;
    color:#1f1f1f;
    text-decoration: none;
    opacity: 0.5;
    pointer-events: auto;
    transition: all .4s;
    transform: rotate(270deg);
  }
  
</style>
    </head>
    <body class="vscode-body vscode-light">
        <!-- title: System Performance -->
<h1>System Performance Monitoring</h1>
<p><a href="#" class="to-top">➤</i></a></p>
<ul>
<li><a href="#methodology-and-concept">Methodology and Concept</a>
<ul>
<li><a href="#use-method">USE method</a></li>
<li><a href="#top-down-microarchitecture-analysis-tma">Top-Down Microarchitecture Analysis (TMA)</a>
<ul>
<li><a href="#tma-in-perf">TMA in <code>perf</code></a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#observability-sources">Observability Sources</a>
<ul>
<li><a href="#proc"><code>/proc</code></a></li>
<li><a href="#sys"><code>/sys</code></a></li>
<li><a href="#tracepoints">Tracepoints</a></li>
<li><a href="#hardware-counters-pmcs">Hardware Counters (PMCs)</a></li>
<li><a href="#kprobe">kprobe</a></li>
</ul>
</li>
<li><a href="#cpu">CPU</a>
<ul>
<li><a href="#interconnect">Interconnect</a></li>
</ul>
</li>
<li><a href="#memory">Memory</a>
<ul>
<li><a href="#swap">Swap</a></li>
</ul>
</li>
<li><a href="#netowrk">Netowrk</a>
<ul>
<li><a href="#linux-network-monitoring-and-tuning">Linux network Monitoring and Tuning</a></li>
</ul>
</li>
</ul>
<h1 id="methodology-and-concept">Methodology and Concept</h1>
<p><img src="images/2022-04-28-09-15-25.png" alt=""></p>
<ul>
<li>Applications and the kernel typically provide data on their state and activity: operation counts,
byte counts, latency measurements, resource utilization, and error rates. They are typically
implemented as integer variables called <em>counters</em>.</li>
<li>These cumulative counters can be read at different times by performance tools for calculating <em>statistics</em>.
<ul>
<li>For example, the <code>vmstat(8)</code> utility prints a system-wide summary of virtual memory statistics and more, based on kernel counters in the <code>/proc</code> file system.</li>
</ul>
</li>
<li>A <em>metric</em> is a statistic that has been selected to evaluate or monitor a target. Most companies use
monitoring agents to record selected statistics (metrics) at regular intervals, and chart them in a
graphical interface to see changes over time.</li>
<li>Monitoring software can also support creating custom <em>alerts</em> from these metrics. Also, alerts may be generated by any layer.</li>
</ul>
<h2 id="use-method">USE method</h2>
<p>The utilization, saturation, and errors (USE) method. It is a methodology that focuses on
system resources and can be summarized as:</p>
<pre><code class="language-bash">For every resource, check utilization, saturation, and errors.
</code></pre>
<table>
<thead>
<tr>
<th>component</th>
<th>type</th>
<th>metric</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>utilization</td>
<td>system-wide: <br> <code>vmstat 1</code>, &quot;us&quot; + &quot;sy&quot; + &quot;st&quot;; <br> <code>sar -u</code>, sum fields except &quot;%idle&quot; and &quot;%iowait&quot;; <br> <code>dstat -c</code>, sum fields except &quot;idl&quot; and &quot;wai&quot;; <br><br> per-cpu: <br> <code>mpstat -P ALL 1</code>, sum fields except &quot;%idle&quot; and &quot;%iowait&quot;; <br><code>sar -P ALL</code>, same as mpstat; <br><br>per-process: <br> <code>top, &quot;%CPU&quot;</code>;<br> <code>htop, &quot;CPU%&quot;</code>; <br> <code>ps -o pcpu</code>; <br> <code>pidstat 1, &quot;%CPU&quot;</code>; <br><br>per-kernel-thread:<br> <code>top/htop</code> (&quot;K&quot; to toggle), where VIRT == 0 (heuristic).</td>
</tr>
<tr>
<td>CPU</td>
<td>saturation</td>
<td>system-wide: <br> <code>vmstat 1, &quot;r&quot;</code> &gt; CPU count ; <br> <code>sar -q, &quot;runq-sz&quot;</code> &gt; CPU count; <br> <code>dstat -p, &quot;run&quot;</code> &gt; CPU count; <br><br> per-process: <br> <code>/proc/PID/schedstat</code> 2nd field (sched_info.run_delay); <br> perf sched latency (shows &quot;Average&quot; and &quot;Maximum&quot; delay per-schedule); <br>dynamic tracing, eg, SystemTap schedtimes.stp &quot;queued(us)&quot; [3]</td>
</tr>
<tr>
<td>CPU</td>
<td>errors</td>
<td>perf (LPE, Linux Performance Events, aka perf_events, from perf tool) if processor specific error events (CPC) are available;</td>
</tr>
<tr>
<td>Memory capacity</td>
<td>utilization</td>
<td>system-wide:<br> <code>free -m</code>, &quot;Mem:&quot; (main memory), &quot;Swap:&quot; (virtual memory); <br><code>vmstat 1</code>, &quot;free&quot; (main memory), &quot;swap&quot; (virtual memory); <br><code>sar -r, &quot;%memused&quot;</code>; <br><code>dstat -m</code>, &quot;free&quot;; <br> <code>slabtop -s c</code> for kmem slab usage; <br><br>per-process:<br> <code>top/htop</code>, &quot;RES&quot; (resident main memory), &quot;VIRT&quot; (virtual memory), &quot;Mem&quot; for system-wide summary</td>
</tr>
<tr>
<td>Memory capacity</td>
<td>saturation</td>
<td>system-wide: <br><code>vmstat 1</code>, &quot;si&quot;/&quot;so&quot; (swapping); <br><code>sar -B</code>, &quot;pgscank&quot; + &quot;pgscand&quot; (scanning); <br><code>sar -W</code>; <br><br>per-process: <br> <code>/proc/PID/stat</code>, 10th field (min_flt) for minor-fault rate <br> dynamic tracing [5];<br><br> OOM killer: <br>`dmesg</td>
</tr>
<tr>
<td>Memory capacity</td>
<td>errors</td>
<td>dmesg for physical failures; dynamic tracing, eg, SystemTap uprobes for failed malloc()s</td>
</tr>
<tr>
<td>Network Interfaces</td>
<td>utilization</td>
<td><code>sar -n DEV 1</code>, &quot;rxKB/s&quot;/max &quot;txKB/s&quot;/max; <br><code>ip -s link</code>, RX/TX tput / max bandwidth; <br><code>/proc/net/dev</code>, &quot;bytes&quot; RX/TX tput/max; <br><code>nicstat &quot;%Util&quot;</code> [6]</td>
</tr>
<tr>
<td>Network Interfaces</td>
<td>saturation</td>
<td><code>ifconfig</code>, &quot;overruns&quot;, &quot;dropped&quot;; <br><code>netstat -s</code>, &quot;segments retransmited&quot;;<br> <code>sar -n EDEV</code>, *drop and *fifo metrics; <br><code>/proc/net/dev</code>, RX/TX &quot;drop&quot;; <br><code>nicstat &quot;Sat&quot;</code> [6];<br> dynamic tracing for other TCP/IP stack queueing [7]</td>
</tr>
<tr>
<td>Network Interfaces</td>
<td>rrors</td>
<td><code>ifconfig</code>, &quot;errors&quot;, &quot;dropped&quot;; <br><code>netstat -i</code>, &quot;RX-ERR&quot;/&quot;TX-ERR&quot;; <br><code>ip -s link</code>, &quot;errors&quot;; <br><code>sar -n EDEV</code>, &quot;rxerr/s&quot; &quot;txerr/s&quot;; <br><code>/proc/net/dev</code>, &quot;errs&quot;, &quot;drop&quot;;<br> extra counters may be under <code>/sys/class/net/...</code>; <br>dynamic tracing of driver function returns</td>
</tr>
<tr>
<td>Storage device I/O</td>
<td>utilization</td>
<td>system-wide: <br><code>iostat -xz 1</code>, &quot;%util&quot;; <br><code>sar -d</code>, &quot;%util&quot;; <br><br>per-process: <br><code>iotop</code>;<br> <code>pidstat -d</code>; <br><code>/proc/PID/sched &quot;se.statistics.iowait_sum&quot;</code></td>
</tr>
<tr>
<td>Storage device I/O</td>
<td>saturation</td>
<td><code>iostat -xnz 1</code>, &quot;avgqu-sz&quot; &gt; 1, or high &quot;await&quot;; <br><code>sar -d</code> same; <br>LPE block probes for queue length/latency; <br>dynamic/static tracing of I/O subsystem (incl. LPE block probes)</td>
</tr>
<tr>
<td>Storage device I/O</td>
<td>errors</td>
<td><code>/sys/devices/.../ioerr_cnt</code>; <br>smartctl; <br>dynamic/static tracing of I/O subsystem response codes [8]</td>
</tr>
<tr>
<td>Storage capacity</td>
<td>utilization</td>
<td>swap:<br> <code>swapon -s</code>; <br><code>free</code>; <code>/proc/meminfo</code> &quot;SwapFree&quot;/&quot;SwapTotal&quot;; <br><br>file systems:<br> <code>&quot;df -h&quot;</code></td>
</tr>
<tr>
<td>Storage capacity</td>
<td>saturation</td>
<td>not sure this one makes sense - once it's full, ENOSPC</td>
</tr>
<tr>
<td>Storage capacity</td>
<td>errors</td>
<td><code>strace</code> for ENOSPC; <br>dynamic tracing for ENOSPC; <br><code>/var/log/messages</code> errs, depending on FS</td>
</tr>
</tbody>
</table>
<br>
<br>
<h2 id="top-down-microarchitecture-analysis-tma">Top-Down Microarchitecture Analysis (TMA)</h2>
<p>TMA observes the execution of the program by collecting specific metrics (ratios of PMCs). Based on those metrics, it characterizes application by relating it to one of the four high-level buckets.</p>
<ul>
<li>initially, we collect metrics for four main buckets: <code>Front End Bound</code>, <code>Back End Bound</code>, <code>Retiring</code>, <code>Bad Speculation</code>.</li>
<li>Say, we found out that the big portion of the program execution was stalled by memory accesses (which is a Back End Bound bucket)</li>
<li>The next step is to run the workload again and collect metrics specific for the Memory Bound bucket only</li>
<li>The process is repeated until we know the exact root cause, for example, <code>L3 Bound</code>.</li>
<li>Analysis tools such as Intel VTune Profiler, AMD uprof, and Linux perf can calculate all the metrics with a single run of the benchmark.</li>
</ul>
<img src="images/2023-06-25-22-20-36.png" width="800">
<h3 id="tma-in-perf">TMA in <code>perf</code></h3>
<p>As of Linux kernel 4.8, perf has an option <code>--topdown</code> used in <code>perf stat</code> command that prints TMA level 1 metrics, i.e., only four high-level buckets:</p>
<ul>
<li>To get values for high-level TMA metrics, Linux perf requires profiling the whole system (-a). This is why we see metrics for all cores. But since we pinned the benchmark to core0 with taskset -c 0, we can only focus on the first row that corresponds to S0-C0.</li>
<li>To get access to Top-Down metrics level 2, 3, etc. one can use <code>toplev</code> tool that is a part of <code>pmu-tools</code></li>
</ul>
<pre><code>$ perf stat --topdown -a -- taskset -c 0 ./7zip-benchmark b
retiring bad speculat FE bound BE bound
S0-C0 30.8% 41.8% 8.8% 18.6% &lt;==
S0-C1 17.4% 2.3% 12.0% 68.2%
S0-C2 10.1% 5.8% 32.5% 51.6%
S0-C3 47.3% 0.3% 2.9% 49.6%
...
</code></pre>
<p>Examples using  <code>toplev</code> tool</p>
<pre><code># First get level-1 metrics:
$ ~/pmu-tools/toplev.py --core S0-C0 -l1 -v --no-desc taskset -c 0 ./a.out
...
# Level 1
S0-C0 Frontend_Bound: 13.81 % Slots
S0-C0 Bad_Speculation: 0.22 % Slots
S0-C0 Backend_Bound: 53.43 % Slots &lt;==
S0-C0 Retiring: 32.53 % Slots

# L1 is backend_bound, drill down one level or more, below is TMA l3 
$ ~/pmu-tools/toplev.py --core S0-C0 -l3 -v --no-desc taskset -c 0 ./a.out
...
# Level 1
S0-C0 Frontend_Bound: 13.91 % Slots
S0-C0 Bad_Speculation: 0.24 % Slots
S0-C0 Backend_Bound: 53.36 % Slots
S0-C0 Retiring: 32.41 % Slots
# Level 2
S0-C0 FE_Bound.FE_Latency: 12.10 % Slots
S0-C0 FE_Bound.FE_Bandwidth: 1.85 % Slots
S0-C0 BE_Bound.Memory_Bound: 44.58 % Slots
S0-C0 BE_Bound.Core_Bound: 8.78 % Slots
# Level 3
S0-C0-T0 BE_Bound.Mem_Bound.L1_Bound: 4.39 % Stalls
S0-C0-T0 BE_Bound.Mem_Bound.L2_Bound: 2.42 % Stalls
S0-C0-T0 BE_Bound.Mem_Bound.L3_Bound: 5.75 % Stalls
S0-C0-T0 BE_Bound.Mem_Bound.DRAM_Bound: 47.11 % Stalls &lt;==
S0-C0-T0 BE_Bound.Mem_Bound.Store_Bound: 0.69 % Stalls
S0-C0-T0 BE_Bound.Core_Bound.Divider: 8.56 % Clocks

# we can confirm with perf
$ perf stat -e cycles,cycle_activity.stalls_l3_miss -- ./a.out
32226253316 cycles
19764641315 cycle_activity.stalls_l3_miss
</code></pre>
<p>Step 2: Locate the place in the code</p>
<pre><code>$ perf record -e cpu/event=0xd1,umask=0x20,name=MEM_LOAD_RETIRED.L3_MISS/ppp ./a.out
$ perf report -n --stdio
...
# Samples: 33K of event ‘MEM_LOAD_RETIRED.’L3_MISS
# Event count (approx.): 71363893
# Overhead Samples Shared Object Symbol
# ........ ......... ................. .................
#
99.95% 33811 a.out [.] foo
0.03% 52 [kernel] [k] get_page_from_freelist
0.01% 3 [kernel] [k] free_pages_prepare
0.00% 1 [kernel] [k] free_pcppages_bulk


$ perf annotate --stdio -M intel foo
Percent | Disassembly of a.out for MEM_LOAD_RETIRED.L3_MISS
------------------------------------------------------------
: Disassembly of section .text:
::
0000000000400a00 &lt;foo&gt;:
: foo():
0.00 : 400a00: nop DWORD PTR [rax+rax*1+0x0]
0.00 : 400a08: nop DWORD PTR [rax+rax*1+0x0]
...
100.00 : 400e07: mov rax,QWORD PTR [rdi+rsi*1] &lt;==
...
0.00 : 400e13: xor rax,rax
0.00 : 400e16: ret

</code></pre>
<br>
<br>
<h1 id="observability-sources">Observability Sources</h1>
<h2 id="proc"><a href="kernel.html#proc-virtual-file-system"><code>/proc</code></a></h2>
<h2 id="sys"><a href="kernel.html#sys-virtual-file-system"><code>/sys</code></a></h2>
<h2 id="tracepoints">Tracepoints</h2>
<p>Tracepoints are a Linux kernel event source based on <em>static instrumentation</em>. Tracepoints are hard-coded instrumentation points placed at logical locations in kernel code.</p>
<p>Available tracepoints can be listed using the <code>perf list</code> command.</p>
<pre><code class="language-bash"><span class="hljs-comment"># the following command traces the block:block_rq_issue tracepoint and prints events live:</span>
$ perf trace -e block:block_rq_issue

<span class="hljs-comment"># trace block I/O issue events only when the size (bytes argument) is larger than 65536</span>
<span class="hljs-comment"># The --filter argument for perf trace was added in Linux 5.5.</span>
$ perf trace -e block:block_rq_issue --filter <span class="hljs-string">&#x27;bytes &gt; 65536&#x27;</span>
</code></pre>
<p>Each tracepoint has a format string that contains event arguments: extra context about the event. The structure of this format string can be seen in a “format” file under <code>/sys/kernel/debug/tracing/events</code>. For example:</p>
<pre><code class="language-bash">$ <span class="hljs-built_in">cat</span> /sys/kernel/debug/tracing/events/block/block_rq_issue/format
</code></pre>
<p><strong>Overhead</strong></p>
<p>When tracepoints are activated, they add a small amount of CPU overhead to each event. The
tracing tool may also add CPU overhead to post-process events, plus file system overheads
to record them.</p>
<p>event rates of less than 10,000 per second cost negligible overhead, and only over 100,000 does the overhead begin to become measurable.</p>
<p>the minimum tracepoint overhead to be <strong>96 nanoseconds</strong> of CPU time.</p>
<p>A disabled tracepoint becomes a small number of instructions: for
x86_64 it is a 5-byte no-operation (nop) instruction. There is also a tracepoint handler added
to the end of the function, which increases its text size a little.</p>
<br>
<h2 id="hardware-counters-pmcs">Hardware Counters (PMCs)</h2>
<p>modern CPU has <strong>Performance Monitoring Unit (PMU)</strong>.</p>
<ul>
<li>It incorporates features that help developers in analyzing the performance of their applications.</li>
<li>Most modern PMUs have a set of <strong>Performance Monitoring Counters (PMC)</strong> that can be used to collect various <strong>Performance events</strong>.</li>
<li>the number of available PMCs is much smaller than the number of performance events. analysis tools uses time multiplexing between groups of performance events during the execution of a program.</li>
<li>there could be other features that enhance performance analysis, like LBR, PEBS, and PT</li>
</ul>
<img src="images/2023-06-25-12-18-49.png" width="600">
<p>A complete list of performance events for all Intel CPU generations can be found in Intel® 64 and IA-32 Architectures Software Developer Manuals, Volume 3B, Chapter 19.</p>
<p><strong>Performance monitoring counters (PMCs)</strong>, (AKA, <em>CPU performance counters</em> (CPCs),
<em>performance instrumentation counters</em> (PICs), or <em>performance monitoring unit events</em> (PMU events)),
they all refer to the same thing: programmable hardware registers on the processor that provide
low-level performance information at the CPU cycle level.</p>
<p>They typically include counters for the following:</p>
<ul>
<li>CPU cycles: Including stall cycles and types of stall cycles</li>
<li>CPU instructions: Retired (executed)</li>
<li>Level 1, 2, 3 cache accesses: Hits, misses</li>
<li>Floating-point unit: Operations</li>
<li>Memory I/O: Reads, writes, stall cycles</li>
<li>Resource I/O: Reads, writes, stall cycles</li>
</ul>
<p>Typically, PMCs are 48 bit wide, which allows analysis tools to run longer without interrupting the program’s
execution. Performance counters are HW registers implemented as Model Specific Registers (MSR).</p>
<ul>
<li>use <code>cpuid</code> to query available PMC</li>
<li>PMCs are accessible via the <code>RDMSR</code> and <code>WRMSR</code> instructions, which can only be executed from kernel space.</li>
<li>Intel CPU often have four fully programmable counters and three fixed-function counters (count core clocks, reference clocks, and instructions retired) per logical core.</li>
</ul>
<p>On Linux PMCs are accessed via the <code>perf_event_open(2)</code> syscall and are consumed by tools including <code>perf(1)</code>.</p>
<p>PMCs can be used in different modes:</p>
<ul>
<li><em>counting</em>, where they count events with practically zero overhead;
<ul>
<li>perf(1) can perform counting using the <code>stat</code> subcommand</li>
</ul>
</li>
<li><em>overflow sampling</em>, where an interrupt is raised for one in every configurable number of events, so that state can be captured.
<ul>
<li>perf(1) can perform sampling using the <code>record</code> subcommand.</li>
<li>Overflow sampling may not record the correct instruction pointer that triggered the event, due
to interrupt latency (often called “skid”) or out-of-order instruction execution.
<ul>
<li>The solution is processor support for what are known as <em>precise events</em>. On Intel, precise events use a technology called precise event-based sampling (PEBS), which uses hardware buffers to record a more accurate (“precise”) instruction pointer at the time of the PMC event. On AMD, precise events use instruction-based sampling (IBS). The Linux perf(1) command supports precise events.</li>
</ul>
</li>
</ul>
</li>
</ul>
<br>
<h2 id="kprobe">kprobe</h2>
<p>kprobes (short for kernel probes) is a Linux kernel event source for tracers based on <em>dynamic
instrumentation</em>. kprobes can trace any kernel function or instruction. They are considered an unstable API because they expose raw kernel functions and arguments that may change between kernel versions.</p>
<p>kprobes are important because they are a last-resort source of virtually unlimited information about kernel behavior in production.</p>
<table>
<thead>
<tr>
<th>Detail</th>
<th>kprobes</th>
<th>Tracepoints</th>
</tr>
</thead>
<tbody>
<tr>
<td>Type</td>
<td>Dynamic</td>
<td>Static</td>
</tr>
<tr>
<td>Rough Number of Events</td>
<td>50,000+</td>
<td>1,000+</td>
</tr>
<tr>
<td>Kernel Maintenance</td>
<td>None</td>
<td>Required</td>
</tr>
<tr>
<td>Disabled Overhead</td>
<td>None</td>
<td>Tiny (NOPs + metadata)</td>
</tr>
<tr>
<td>Stable API</td>
<td>No</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p><strong>Overhead</strong></p>
<p>For a particular system I measured the minimum kprobe CPU cost to be 76 nanoseconds, and the minimum kretprobe CPU cost to be 212 nanoseconds [Gregg 19].</p>
<br>
<br>
<h1 id="cpu">CPU</h1>
<p><strong>CPU Priority inversion</strong></p>
<p>Priority inversion occurs when a lower-priority thread holds a resource and blocks a higher priority
thread from running.</p>
<p>This can be solved using a priority inheritance scheme. Linux since 2.6.18 has provided a user-level mutex that supports priority inheritance</p>
<p>Here is an example of how this can work:</p>
<ol>
<li>Thread A performs monitoring and has a low priority. It acquires an address space lock for
a production database, to check memory usage.</li>
<li>Thread B, a routine task to perform compression of system logs, begins running.</li>
<li>There is insufficient CPU to run both. Thread B preempts A and runs.</li>
<li>Thread C is from the production database, has a high priority, and has been sleeping waiting
for I/O. This I/O now completes, putting thread C back into the runnable state.</li>
<li>Thread C preempts B, runs, but then blocks on the address space lock held by thread A.
Thread C leaves CPU.</li>
<li>The scheduler picks the next-highest-priority thread to run: B.</li>
<li>With thread B running, a high-priority thread, C, is effectively blocked on a lower-priority
thread, B. This is priority inversion.</li>
<li>Priority inheritance gives thread A thread C’s high priority, preempting B, until it releases
the lock. Thread C can now run.</li>
</ol>
<h2 id="interconnect">Interconnect</h2>
<p>For multiprocessor architectures, processors are connected using either a shared system bus or a
dedicated interconnect. Modern servers are typically multiprocessor, NUMA, and use a CPU interconnect instead.</p>
<p>Example interconnects include Intel’s Quick Path Interconnect (<strong>QPI, being replaced by UPI</strong>), Intel’s Ultra Path Interconnect (UPI), AMD’s HyperTransport (HT), ARM’s CoreLink Interconnects (there are three different types), and IBM’s Coherent Accelerator Processor Interface (CAPI).</p>
<ul>
<li>QPI's cache coherency mode could be tuned in the BIOS, with options including Home Snoop to optimize for memory bandwidth, Early Snoop to optimize for memory latency, and Directory Snoop to improve scalability (it involves tracking what is shared).</li>
<li>UPI only supports Directory Snoop.</li>
</ul>
<h1 id="memory">Memory</h1>
<p><code>vmstat</code></p>
<pre><code class="language-bash">vmstat 1

vmstat -Sm 1 <span class="hljs-comment"># output units as megabytes using the -S option</span>
vmstat -a 1  <span class="hljs-comment"># -a option to print a breakdown of inactive and active memory</span>
</code></pre>
<p><code>sar</code></p>
<pre><code class="language-bash">sar -B 1 <span class="hljs-comment"># Paging statistics</span>
<span class="hljs-comment"># pgscank column: page scanned by kswapd daemon per second.</span>
<span class="hljs-comment"># pgscand column: effectively shows the rate at which an application is blocking on memory allocations and entering direct reclaim (higher is bad).</span>
<span class="hljs-comment"># %vmeff column: a measure of page reclaim efficiency. High means pages are successfully stolen from the inactive list (healthy); low means the system is struggling.</span>

sar -H 1 <span class="hljs-comment"># Huge pages statistics</span>
sar -r [ALL] 1 <span class="hljs-comment"># Memory utilization, ALL flag to print more stats.</span>
sar -S 1 <span class="hljs-comment"># Swap space statistics</span>
sar -W 1 <span class="hljs-comment"># Swapping statistics</span>
</code></pre>
<p><code>numastat</code> - show memory usage on NUMA system, per CPU socket. It can show system-wide and <strong>per-process</strong>.</p>
<pre><code class="language-bash">numastat <span class="hljs-comment"># show memory allocation on node</span>
<span class="hljs-comment"># numa_hit: Memory allocations on the intended NUMA node.</span>
<span class="hljs-comment"># numa_miss + numa_foreign: Memory allocations not on the preferred NUMA node. (numa_miss shows local allocations that should have been elsewhere, and numa_foreign shows remote allocations that should have been local.)</span>
<span class="hljs-comment"># other_node: Memory allocations on this node while the process was running elsewhere.</span>

numastat -m      <span class="hljs-comment"># -m print output in the style of /proc/meminfo</span>
numastat -mp PID <span class="hljs-comment"># -p print per-process info</span>
</code></pre>
<h2 id="swap">Swap</h2>
<p><code>swapon(1)</code> can show whether swap devices have been configured and how much of their volume is in use.</p>
<pre><code class="language-bash">$ swapon
NAME        TYPE        SIZE    USED    PRIO
/dev/dm-2   partition   980M    611.6M  -2
/swap1      file        30G     10.9M   -3
</code></pre>
<p><code>/proc/meminfo</code></p>
<pre><code class="language-bash">$ <span class="hljs-built_in">cat</span> /proc/meminfo | grep -i swap
SwapCached:     0KB
SwapTotal:      9764860KB
SwapFree:       9764860KB
</code></pre>
<p>Check active swap usage using <code>vmstat 1</code></p>
<ul>
<li><strong><code>swpd</code></strong> column: Amount of swapped-out memory</li>
<li><strong><code>si</code></strong> column: Memory swapped in (paging)</li>
<li><strong><code>so</code></strong> column: Memory swapped out (paging)</li>
</ul>
<br>
<br>
<h1 id="netowrk">Netowrk</h1>
<h2 id="linux-network-monitoring-and-tuning"><a href="tcp.html#linux-network-monitoring-and-tuning">Linux network Monitoring and Tuning</a></h2>
<br>
<br>
        <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
        
    </body>
    </html>